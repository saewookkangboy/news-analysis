"""
íƒ€ê²Ÿ ë¶„ì„ ì„œë¹„ìŠ¤
AIë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ, ì˜¤ë””ì–¸ìŠ¤, ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""
import os
import logging
import re
from typing import Optional, Dict, Any, AsyncGenerator
import json

from backend.config import settings

# Vercel í™˜ê²½ í™•ì¸
IS_VERCEL = os.environ.get("VERCEL") == "1"
from backend.services.progress_tracker import ProgressTracker
from backend.utils.token_optimizer import (
    optimize_prompt, estimate_tokens, get_max_tokens_for_model, optimize_additional_context,
    extract_and_fix_json, parse_json_with_fallback
)
from backend.utils.result_normalizer import normalize_analysis_result, ensure_result_structure
from backend.utils.gemini_utils import (
    generate_content_with_fallback,
    generate_content_stream_with_fallback,
    build_model_candidates,
    is_model_not_found_error,
)
from backend.utils.security import (
    get_api_key_safely,
    check_api_keys_status,
    log_api_key_status_safely,
    validate_api_key
)

logger = logging.getLogger(__name__)


async def analyze_target(
    target_keyword: str,
    target_type: str = "keyword",
    additional_context: Optional[str] = None,
    use_gemini: bool = False,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> Dict[str, Any]:
    """
    íƒ€ê²Ÿ ë¶„ì„ ìˆ˜í–‰
    
    Args:
        target_keyword: ë¶„ì„í•  íƒ€ê²Ÿ í‚¤ì›Œë“œ ë˜ëŠ” ì£¼ì œ
        target_type: ë¶„ì„ ìœ í˜• (keyword, audience, comprehensive)
        additional_context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        use_gemini: Gemini API ì‚¬ìš© ì—¬ë¶€ (Trueì¼ ê²½ìš° OpenAI + Gemini ë³´ì™„ ë¶„ì„)
        start_date: ë¶„ì„ ì‹œì‘ì¼ (YYYY-MM-DD í˜•ì‹)
        end_date: ë¶„ì„ ì¢…ë£Œì¼ (YYYY-MM-DD í˜•ì‹)
        progress_tracker: ì§„í–‰ ìƒí™© ì¶”ì  ê°ì²´
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    try:
        logger.info(f"íƒ€ê²Ÿ ë¶„ì„ ì‹œì‘: {target_keyword} (íƒ€ì…: {target_type}, Gemini ë³´ì™„: {use_gemini})")
        
        # API í‚¤ ìƒíƒœ í™•ì¸ (ë³´ì•ˆ ê°•í™”: í‚¤ ê°’ì€ ë¡œê¹…í•˜ì§€ ì•ŠìŒ)
        openai_key = get_api_key_safely('OPENAI_API_KEY')
        gemini_key = get_api_key_safely('GEMINI_API_KEY')
        
        has_openai_key = bool(openai_key)
        has_gemini_key = bool(gemini_key)
        
        # ì•ˆì „í•œ ë¡œê¹… (í‚¤ ê°’ì€ ë…¸ì¶œí•˜ì§€ ì•ŠìŒ)
        logger.info("=" * 60)
        logger.info("API í‚¤ ìƒíƒœ í™•ì¸")
        log_api_key_status_safely('OPENAI_API_KEY', has_openai_key)
        log_api_key_status_safely('GEMINI_API_KEY', has_gemini_key)
        logger.info("=" * 60)
        
        if not has_openai_key and not has_gemini_key:
            logger.error("âš ï¸ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤! ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜ë©ë‹ˆë‹¤.")
            logger.error("í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEY ë˜ëŠ” GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            if progress_tracker:
                await progress_tracker.update(100, "AI API í‚¤ ë¯¸ì„¤ì • - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")
            return _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
        
        # ê¸°ë³¸ì ìœ¼ë¡œ OpenAI API ì‚¬ìš©
        if has_openai_key:
            if progress_tracker:
                await progress_tracker.update(10, "OpenAI APIë¡œ ê¸°ë³¸ ë¶„ì„ ì‹œì‘...")
            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œë§Œ ìƒì„¸ ë¡œê¹…
            if settings.LOG_LEVEL == "DEBUG":
                logger.debug("=" * 60)
                logger.debug("ğŸš€ OpenAI API í˜¸ì¶œ ì‹œì‘")
                logger.debug(f"API í‚¤: âœ… ì„¤ì •ë¨")
                logger.debug(f"ëª¨ë¸: {settings.OPENAI_MODEL}")
                logger.debug("=" * 60)
            else:
                logger.info("OpenAI API í˜¸ì¶œ ì‹œì‘")
            try:
                result = await _analyze_with_openai(
                    target_keyword, target_type, additional_context, start_date, end_date, progress_tracker
                )
                if settings.LOG_LEVEL == "DEBUG":
                    logger.debug("=" * 60)
                    logger.debug("âœ… OpenAI API ë¶„ì„ ì„±ê³µ ì™„ë£Œ")
                    logger.debug(f"ê²°ê³¼ í‚¤: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                    logger.debug("=" * 60)
                else:
                    logger.info("OpenAI API ë¶„ì„ ì„±ê³µ ì™„ë£Œ")
            except ValueError as ve:
                # API í‚¤ ê´€ë ¨ ì˜¤ë¥˜ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
                logger.error(f"âŒ OpenAI API í‚¤ ì˜¤ë¥˜: {ve}", exc_info=True)
                raise
            except Exception as e:
                logger.error("=" * 60)
                logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
                # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
                if not IS_VERCEL:
                    import traceback
                    logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                else:
                    logger.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì„œë²„ ë¡œê·¸ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                logger.error("=" * 60)
                # OpenAI ì‹¤íŒ¨ ì‹œ Geminië¡œ ì¬ì‹œë„
                if has_gemini_key:
                    logger.info("ğŸ”„ Gemini APIë¡œ ì¬ì‹œë„ ì¤‘...")
                    try:
                        if progress_tracker:
                            await progress_tracker.update(50, "OpenAI ì‹¤íŒ¨, Geminië¡œ ì¬ì‹œë„ ì¤‘...")
                        result = await _analyze_with_gemini(
                            target_keyword, target_type, additional_context, start_date, end_date, progress_tracker
                        )
                        logger.info("âœ… Gemini API ë¶„ì„ ì„±ê³µ (OpenAI ì‹¤íŒ¨ í›„ ì¬ì‹œë„)")
                    except Exception as e2:
                        logger.error(f"âŒ Gemini API ì¬ì‹œë„ë„ ì‹¤íŒ¨: {type(e2).__name__}: {e2}", exc_info=True)
                        logger.error("âš ï¸ ëª¨ë“  AI API í˜¸ì¶œ ì‹¤íŒ¨ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜")
                        if progress_tracker:
                            await progress_tracker.update(100, "ëª¨ë“  AI API ì‹¤íŒ¨ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")
                        return _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
                else:
                    logger.error("âš ï¸ OpenAI ì‹¤íŒ¨ ë° Gemini API í‚¤ ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜")
                    if progress_tracker:
                        await progress_tracker.update(100, "OpenAI ì‹¤íŒ¨, Gemini ì—†ìŒ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")
                    return _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
            
            # Gemini APIê°€ ì„ íƒë˜ê³  ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°, OpenAI ê²°ê³¼ë¥¼ ë³´ì™„
            if use_gemini and has_gemini_key:
                try:
                    if progress_tracker:
                        await progress_tracker.update(60, "Gemini APIë¡œ ê²°ê³¼ ë³´ì™„ ì¤‘...")
                    if settings.LOG_LEVEL == "DEBUG":
                        logger.debug("=" * 60)
                        logger.debug("ğŸ”„ Gemini APIë¡œ ê²°ê³¼ ë³´ì™„ ì‹œì‘")
                        logger.debug("=" * 60)
                    else:
                        logger.info("Gemini APIë¡œ ê²°ê³¼ ë³´ì™„ ì‹œì‘")
                    gemini_result = await _analyze_with_gemini(
                        target_keyword, target_type, additional_context, start_date, end_date, progress_tracker
                    )
                    # OpenAIì™€ Gemini ê²°ê³¼ í†µí•©
                    if progress_tracker:
                        await progress_tracker.update(85, "OpenAI + Gemini ê²°ê³¼ í†µí•© ì¤‘...")
                    result = _merge_analysis_results(result, gemini_result, target_type)
                    if settings.LOG_LEVEL == "DEBUG":
                        logger.debug("=" * 60)
                        logger.debug("âœ… OpenAI + Gemini ê²°ê³¼ í†µí•© ì™„ë£Œ")
                        logger.debug("=" * 60)
                    else:
                        logger.info("OpenAI + Gemini ê²°ê³¼ í†µí•© ì™„ë£Œ")
                except Exception as e:
                    logger.warning("=" * 60)
                    logger.warning(f"âš ï¸ Gemini API ë³´ì™„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (OpenAI ê²°ê³¼ë§Œ ì‚¬ìš©): {type(e).__name__}: {e}")
                    # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
                    if not IS_VERCEL:
                        import traceback
                        logger.warning(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                    logger.warning("=" * 60)
                    # Gemini ì‹¤íŒ¨í•´ë„ OpenAI ê²°ê³¼ëŠ” ìœ ì§€
                    if progress_tracker:
                        await progress_tracker.update(90, "Gemini ë³´ì™„ ì‹¤íŒ¨, OpenAI ê²°ê³¼ë§Œ ì‚¬ìš©")
        elif use_gemini and has_gemini_key:
            # OpenAIê°€ ì—†ê³  Geminië§Œ ìˆëŠ” ê²½ìš°
            if progress_tracker:
                await progress_tracker.update(10, "Gemini APIë¡œ ë¶„ì„ ì‹œì‘...")
            if settings.LOG_LEVEL == "DEBUG":
                logger.debug("=" * 60)
                logger.debug("ğŸš€ Gemini API í˜¸ì¶œ ì‹œì‘ (OpenAI ì—†ìŒ)")
                logger.debug(f"API í‚¤: âœ… ì„¤ì •ë¨")
                logger.debug("=" * 60)
            else:
                logger.info("Gemini API í˜¸ì¶œ ì‹œì‘")
            try:
                result = await _analyze_with_gemini(
                    target_keyword, target_type, additional_context, start_date, end_date, progress_tracker
                )
                if settings.LOG_LEVEL == "DEBUG":
                    logger.debug("=" * 60)
                    logger.debug("âœ… Gemini API ë¶„ì„ ì„±ê³µ ì™„ë£Œ")
                    logger.debug(f"ê²°ê³¼ í‚¤: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
                    logger.debug("=" * 60)
                else:
                    logger.info("Gemini API ë¶„ì„ ì„±ê³µ ì™„ë£Œ")
            except ValueError as ve:
                # API í‚¤ ê´€ë ¨ ì˜¤ë¥˜ëŠ” ì¬ì‹œë„í•˜ì§€ ì•ŠìŒ
                logger.error(f"âŒ Gemini API í‚¤ ì˜¤ë¥˜: {ve}", exc_info=True)
                raise
            except Exception as e:
                logger.error("=" * 60)
                logger.error(f"âŒ Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
                import traceback
                logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                logger.error("=" * 60)
                logger.error("âš ï¸ Gemini API ì‹¤íŒ¨ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì „í™˜")
                if progress_tracker:
                    await progress_tracker.update(100, "Gemini ì‹¤íŒ¨ - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")
                return _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
        else:
            # AI APIê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„ ìˆ˜í–‰
            logger.warning("âš ï¸ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ ì‚¬ìš©")
            logger.warning("í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEY ë˜ëŠ” GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            if progress_tracker:
                await progress_tracker.update(100, "AI API í‚¤ ë¯¸ì„¤ì • - ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ")
            result = _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
        
        logger.info(f"âœ… íƒ€ê²Ÿ ë¶„ì„ ì™„ë£Œ: {target_keyword}")
        
        # ê²°ê³¼ ì •ê·œí™” (ë¶„ì„ ìœ í˜•ë³„ í‘œì¤€í™”ëœ êµ¬ì¡°ë¡œ ë³€í™˜)
        try:
            normalized_result = normalize_analysis_result(result, target_type)
            normalized_result = ensure_result_structure(normalized_result, target_type)
            logger.info(f"âœ… ê²°ê³¼ ì •ê·œí™” ì™„ë£Œ: {target_type}")
            return normalized_result
        except Exception as e:
            logger.warning(f"âš ï¸ ê²°ê³¼ ì •ê·œí™” ì‹¤íŒ¨ (ì›ë³¸ ê²°ê³¼ ë°˜í™˜): {e}")
            # ì •ê·œí™” ì‹¤íŒ¨í•´ë„ ì›ë³¸ ê²°ê³¼ëŠ” ë°˜í™˜
            return ensure_result_structure(result, target_type)
        
    except ValueError as ve:
        # API í‚¤ ê´€ë ¨ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
        logger.error(f"âŒ API í‚¤ ì˜¤ë¥˜: {ve}")
        raise
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"âŒ íƒ€ê²Ÿ ë¶„ì„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {type(e).__name__}: {e}")
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
        if not IS_VERCEL:
            import traceback
            logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        else:
            logger.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì„œë²„ ë¡œê·¸ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        logger.error("=" * 60)
        # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¼ë„ ë°˜í™˜
        logger.warning("âš ï¸ ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ fallback ì‹œë„")
        try:
            return _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
        except Exception as e2:
            logger.error(f"âŒ ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë„ ì‹¤íŒ¨: {e2}")
            raise


async def _analyze_with_gemini(
    target_keyword: str,
    target_type: str,
    additional_context: Optional[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> Dict[str, Any]:
    """Gemini APIë¥¼ ì‚¬ìš©í•œ ë¶„ì„"""
    try:
        import asyncio
        
        # API í‚¤ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ ì½ê¸° - Vercel í˜¸í™˜ì„±)
        # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ API í‚¤ í™•ì¸ (ìš°ì„ ìˆœìœ„: í™˜ê²½ ë³€ìˆ˜ > Settings)
        api_key = get_api_key_safely('GEMINI_API_KEY')
        
        if not api_key:
            logger.error("GEMINI_API_KEY ë¯¸ì„¤ì •")
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if settings.LOG_LEVEL == "DEBUG":
            logger.debug("=" * 60)
            logger.debug("ğŸš€ Gemini API í˜¸ì¶œ ì‹œì‘")
            logger.debug(f"API í‚¤: âœ… ì„¤ì •ë¨")
            logger.debug(f"ëª¨ë¸: {getattr(settings, 'GEMINI_MODEL', 'gemini-2.0-flash')}")
            logger.debug("=" * 60)
        else:
            logger.info("Gemini API í˜¸ì¶œ ì‹œì‘")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ìµœì í™” (í† í° ìµœì í™” ê°•í™”)
        additional_context_optimized = optimize_additional_context(additional_context, max_length=300)
        prompt = _build_analysis_prompt(target_keyword, target_type, additional_context_optimized, start_date, end_date)
        
        # í† í° ìµœì í™” ì ìš© (ì„¤ì • íŒŒì¼ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°)
        prompt = optimize_prompt(prompt, max_length=getattr(settings, 'PROMPT_MAX_LENGTH', 4000))
        prompt_tokens = estimate_tokens(prompt)
        
        # ëª¨ë¸ ì„¤ì • (ê¸°ë³¸ê°’: gemini-2.5-flash)
        model_name = getattr(settings, 'GEMINI_MODEL', 'gemini-2.0-flash')
        logger.info(f"Gemini API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘... (ëª¨ë¸: {model_name})")
        logger.info(f"í† í° ìµœì í™”: í”„ë¡¬í”„íŠ¸ {prompt_tokens} í† í°, ê¸¸ì´: {len(prompt)} ë¬¸ì")
        
        # ìƒˆë¡œìš´ Gemini API ë°©ì‹ ì‹œë„ (from google import genai)
        try:
            from google import genai
            
            # API í‚¤ ì„¤ì • (ë³´ì•ˆ ìœ í‹¸ë¦¬í‹° ì‚¬ìš©)
            api_key = get_api_key_safely('GEMINI_API_KEY')
            if api_key:
                client = genai.Client(api_key=api_key)
            else:
                # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
                client = genai.Client()
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ í”„ë¡¬í”„íŠ¸ ê²°í•© (ì´ë¯¸ ê°„ì†Œí™”ë¨)
            system_message = _build_system_message(target_type)
            full_prompt = f"{system_message}\n\n{prompt}\n\nJSON only."
            
            # í† í° ìˆ˜ ê³„ì‚° ë° max_tokens ì„¤ì • (ì¶œë ¥ í† í° ì œí•œ)
            full_prompt_tokens = estimate_tokens(full_prompt)
            max_output_tokens = min(get_max_tokens_for_model(model_name, full_prompt_tokens), getattr(settings, 'MAX_OUTPUT_TOKENS', 3000))
            
            # API í˜¸ì¶œ (ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•´ run_in_executor ì‚¬ìš©)
            if settings.LOG_LEVEL == "DEBUG":
                logger.debug("=" * 60)
                logger.debug("ğŸ“¡ Gemini API ìš”ì²­ ì „ì†¡ ì¤‘...")
                logger.debug(f"ëª¨ë¸: {model_name}")
                logger.debug(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(full_prompt)} ë¬¸ì")
                logger.debug(f"í”„ë¡¬í”„íŠ¸ í† í° ì¶”ì •: {full_prompt_tokens}")
                logger.debug(f"ìµœëŒ€ ì¶œë ¥ í† í°: {max_output_tokens}")
                logger.debug("=" * 60)
            else:
                logger.info(f"Gemini API ìš”ì²­ ì „ì†¡ ì¤‘... (ëª¨ë¸: {model_name})")
            try:
                response = await generate_content_with_fallback(
                    client=client,
                    model=model_name,
                    contents=full_prompt,
                    config={
                        "response_mime_type": "application/json",
                        "max_output_tokens": max_output_tokens,
                        "temperature": 0.5,
                    },
                    logger=logger,
                )
                if settings.LOG_LEVEL == "DEBUG":
                    logger.debug("=" * 60)
                    logger.debug("âœ… Gemini API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
                    logger.debug("=" * 60)
                else:
                    logger.info("Gemini API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            except Exception as e:
                logger.error("=" * 60)
                logger.error(f"âŒ Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
                # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
                if not IS_VERCEL:
                    import traceback
                    logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                else:
                    logger.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì„œë²„ ë¡œê·¸ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
                logger.error("=" * 60)
                raise ValueError(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            # ì‘ë‹µ íŒŒì‹±
            result_text = response.text if hasattr(response, 'text') else str(response)
            logger.info(f"Gemini ì‘ë‹µ ê¸¸ì´: {len(result_text)} ë¬¸ì")
            
        except ImportError:
            # ìƒˆë¡œìš´ ë°©ì‹ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‹œë„
            import google.generativeai as genai_old
            
            api_key_old = get_api_key_safely('GEMINI_API_KEY')
            if not api_key_old:
                raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            genai_old.configure(api_key=api_key_old)
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì™€ í”„ë¡¬í”„íŠ¸ ê²°í•© (ìµœì í™”)
            system_message = _build_system_message(target_type)
            full_prompt = f"{system_message}\n\n{prompt}\n\nJSON only."
            
            # í† í° ìˆ˜ ê³„ì‚° ë° max_tokens ì„¤ì • (ì¶œë ¥ í† í° ì œí•œ)
            full_prompt_tokens = estimate_tokens(full_prompt)
            max_output_tokens = min(get_max_tokens_for_model(model_name, full_prompt_tokens), getattr(settings, 'MAX_OUTPUT_TOKENS', 3000))
            
            # API í˜¸ì¶œ (ë¹„ë™ê¸° ì‹¤í–‰ì„ ìœ„í•´ run_in_executor ì‚¬ìš©)
            loop = asyncio.get_event_loop()
            response = None
            last_error = None
            for candidate in build_model_candidates(model_name):
                try:
                    if candidate != model_name:
                        logger.warning(f"GEMINI_MODEL fallback ì‚¬ìš©: {candidate}")
                    model = genai_old.GenerativeModel(candidate)
                    # JSON ì‘ë‹µ ê°•ì œ ì‹œë„ (ê¸°ì¡´ API ë°©ì‹)
                    # google.generativeaiì—ì„œëŠ” generation_config ì‚¬ìš©
                    try:
                        if hasattr(genai_old, 'types') and hasattr(genai_old.types, 'GenerationConfig'):
                            gen_config = genai_old.types.GenerationConfig(
                                response_mime_type="application/json",
                                max_output_tokens=max_output_tokens,
                                temperature=0.5,
                            )
                        else:
                            gen_config = {
                                "response_mime_type": "application/json",
                                "max_output_tokens": max_output_tokens,
                                "temperature": 0.5,
                            }
                        response = await loop.run_in_executor(
                            None,
                            lambda: model.generate_content(
                                full_prompt,
                                generation_config=gen_config,
                            ),
                        )
                    except (AttributeError, TypeError):
                        response = await loop.run_in_executor(
                            None,
                            lambda: model.generate_content(
                                full_prompt,
                                generation_config={
                                    "response_mime_type": "application/json",
                                    "max_output_tokens": max_output_tokens,
                                    "temperature": 0.5,
                                },
                            ),
                        )
                    logger.info("âœ… JSON ëª¨ë“œë¡œ Gemini API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
                    break
                except Exception as e:
                    last_error = e
                    if not is_model_not_found_error(e):
                        raise
                    continue
            if response is None:
                logger.error("=" * 60)
                logger.error(f"âŒ Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {last_error}")
                logger.error("=" * 60)
                raise ValueError(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(last_error)}")
            
            # ì‘ë‹µ íŒŒì‹±
            result_text = response.text if hasattr(response, 'text') else str(response)
            logger.info(f"Gemini ì‘ë‹µ ê¸¸ì´: {len(result_text)} ë¬¸ì (ê¸°ì¡´ ë°©ì‹)")
        
        if progress_tracker:
            await progress_tracker.update(80, "AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘...")
        
        # JSON í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„
        if not result_text:
            raise ValueError("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # ê°•í™”ëœ JSON íŒŒì‹± ì‚¬ìš©
        try:
            result = parse_json_with_fallback(result_text)
            logger.info("âœ… JSON íŒŒì‹± ì„±ê³µ (ê°•í™”ëœ íŒŒì„œ ì‚¬ìš©)")
        except ValueError as e:
            logger.error(f"JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {e}")
            logger.error(f"ì›ë³¸ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì): {result_text[:500] if len(result_text) > 500 else result_text}")
            logger.error(f"ì›ë³¸ í…ìŠ¤íŠ¸ (ë§ˆì§€ë§‰ 500ì): {result_text[-500:] if len(result_text) > 500 else result_text}")
            
            # ìµœì†Œí•œì˜ êµ¬ì¡°ë¼ë„ ë°˜í™˜
            result = {
                "executive_summary": f"{target_keyword}ì— ëŒ€í•œ {target_type} ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. (JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ êµ¬ì¡°ë§Œ ë°˜í™˜)",
                "key_findings": {
                    "primary_insights": [
                        "AI ì‘ë‹µ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        "ì›ë³¸ ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”.",
                        f"ì˜¤ë¥˜: {str(e)[:200]}"
                    ],
                    "quantitative_metrics": {}
                },
                "detailed_analysis": {
                    "insights": {
                        "raw_response": result_text[:2000] if len(result_text) > 2000 else result_text
                    }
                },
                "strategic_recommendations": {
                    "immediate_actions": [
                        "ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ AI ì‘ë‹µì„ ê²€í† í•˜ì„¸ìš”.",
                        "í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•˜ì—¬ JSON í˜•ì‹ ì‘ë‹µì„ ê°•ì œí•˜ì„¸ìš”."
                    ]
                },
                "target_keyword": target_keyword,
                "target_type": target_type,
                "error": "JSON íŒŒì‹± ì‹¤íŒ¨",
                "raw_response_length": len(result_text)
            }
        
        # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        if "target_keyword" not in result:
            result["target_keyword"] = target_keyword
        if "target_type" not in result:
            result["target_type"] = target_type
        
        return result
        
    except ImportError as e:
        logger.error("=" * 60)
        logger.error(f"âŒ Gemini API íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
        logger.error("'pip install google-genai' ë˜ëŠ” 'pip install google-generativeai'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        logger.error("=" * 60)
        raise ValueError(f"Gemini API íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜: {e}")
    except ValueError as ve:
        # API í‚¤ ê´€ë ¨ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
        raise
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"âŒ Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
        if not IS_VERCEL:
            import traceback
            logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        else:
            logger.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì„œë²„ ë¡œê·¸ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        logger.error("=" * 60)
        raise ValueError(f"Gemini API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


async def _analyze_with_openai(
    target_keyword: str,
    target_type: str,
    additional_context: Optional[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> Dict[str, Any]:
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ë¶„ì„"""
    try:
        from openai import AsyncOpenAI
        
        # API í‚¤ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ ì½ê¸° - Vercel í˜¸í™˜ì„±)
        # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ API í‚¤ í™•ì¸ (ìš°ì„ ìˆœìœ„: í™˜ê²½ ë³€ìˆ˜ > Settings)
        api_key = get_api_key_safely('OPENAI_API_KEY')
        
        if not api_key:
            logger.error("OPENAI_API_KEY ë¯¸ì„¤ì •")
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if settings.LOG_LEVEL == "DEBUG":
            logger.debug(f"OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘... (ëª¨ë¸: {settings.OPENAI_MODEL})")
            logger.debug(f"API í‚¤: âœ… ì„¤ì •ë¨")
        else:
            logger.info(f"OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘... (ëª¨ë¸: {settings.OPENAI_MODEL})")
        client = AsyncOpenAI(api_key=api_key)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± ë° ìµœì í™” (í† í° ìµœì í™” ê°•í™”)
        if progress_tracker:
            await progress_tracker.update(20, "í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
        
        # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ìµœì í™” (ë” ì§§ê²Œ)
        additional_context_optimized = optimize_additional_context(additional_context, max_length=300)
        prompt = _build_analysis_prompt(target_keyword, target_type, additional_context_optimized, start_date, end_date)
        
        # í† í° ìµœì í™” ì ìš© (ì„¤ì • íŒŒì¼ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°)
        prompt = optimize_prompt(prompt, max_length=getattr(settings, 'PROMPT_MAX_LENGTH', 4000))
        prompt_tokens = estimate_tokens(prompt)
        
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„± ë° ìµœì í™” (ì´ë¯¸ ê°„ì†Œí™”ë¨)
        system_message = _build_system_message(target_type)
        
        # í† í° ìˆ˜ ê³„ì‚° ë° max_tokens ì„¤ì •
        full_prompt_tokens = estimate_tokens(system_message) + prompt_tokens
        max_output_tokens = get_max_tokens_for_model(settings.OPENAI_MODEL, full_prompt_tokens)
        
        logger.info(f"í† í° ìµœì í™”: í”„ë¡¬í”„íŠ¸ {prompt_tokens} í† í°, ì‹œìŠ¤í…œ {estimate_tokens(system_message)} í† í°, ì´ {full_prompt_tokens} í† í°")
        
        if progress_tracker:
            await progress_tracker.update(30, "OpenAI API ìš”ì²­ ì „ì†¡ ì¤‘...")
        
        # API í˜¸ì¶œ
        if settings.LOG_LEVEL == "DEBUG":
            logger.debug("=" * 60)
            logger.debug("ğŸ“¡ OpenAI API ìš”ì²­ ì „ì†¡ ì¤‘...")
            logger.debug(f"ëª¨ë¸: {settings.OPENAI_MODEL}")
            logger.debug(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            logger.debug(f"í”„ë¡¬í”„íŠ¸ í† í° ì¶”ì •: {full_prompt_tokens}")
            logger.debug(f"ìµœëŒ€ ì¶œë ¥ í† í°: {max_output_tokens}")
            logger.debug("=" * 60)
        else:
            logger.info(f"OpenAI API ìš”ì²­ ì „ì†¡ ì¤‘... (ëª¨ë¸: {settings.OPENAI_MODEL})")
        try:
            response = await client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.5,  # 0.7ì—ì„œ 0.5ë¡œ ë‚®ì¶°ì„œ ë” ë¹ ë¥´ê³  ì¼ê´€ëœ ì‘ë‹µ
                max_tokens=min(max_output_tokens, 4000),  # ìµœëŒ€ ì¶œë ¥ í† í° ì œí•œ (4000ìœ¼ë¡œ ì œí•œí•˜ì—¬ ì†ë„ í–¥ìƒ)
                response_format={"type": "json_object"}  # JSON ì‘ë‹µ ê°•ì œ
            )
            if settings.LOG_LEVEL == "DEBUG":
                logger.debug("=" * 60)
                logger.debug("âœ… OpenAI API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
                logger.debug(f"ì‘ë‹µ ID: {response.id if hasattr(response, 'id') else 'N/A'}")
                logger.debug(f"ì‚¬ìš©ëœ í† í°: {response.usage.total_tokens if hasattr(response, 'usage') else 'N/A'}")
                logger.debug("=" * 60)
            else:
                logger.info("OpenAI API ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
        except Exception as api_error:
            logger.error("=" * 60)
            logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(api_error).__name__}: {api_error}")
            # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
            if not IS_VERCEL:
                import traceback
                logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            else:
                logger.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì„œë²„ ë¡œê·¸ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            logger.error("=" * 60)
            raise ValueError(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(api_error)}")
        
        result_text = response.choices[0].message.content
        
        if not result_text:
            raise ValueError("OpenAI API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        logger.info(f"OpenAI ì‘ë‹µ ê¸¸ì´: {len(result_text)} ë¬¸ì")
        
        if progress_tracker:
            await progress_tracker.update(80, "AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘...")
        
        # ê°•í™”ëœ JSON íŒŒì‹± ì‚¬ìš©
        try:
            if isinstance(result_text, str):
                result = parse_json_with_fallback(result_text)
                if progress_tracker:
                    await progress_tracker.update(90, "JSON íŒŒì‹± ì™„ë£Œ, ê²°ê³¼ ì •ë¦¬ ì¤‘...")
            else:
                # ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°
                result = {
                    "executive_summary": f"{target_keyword}ì— ëŒ€í•œ {target_type} ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                    "key_findings": {
                        "primary_insights": [
                            "AI ì‘ë‹µì´ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.",
                            "JSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        ],
                        "quantitative_metrics": {}
                    },
                    "detailed_analysis": {
                        "insights": {
                            "raw_response": str(result_text)[:500]
                        }
                    },
                    "strategic_recommendations": {
                        "immediate_actions": [
                            "AI ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.",
                            "JSON í˜•ì‹ ì‘ë‹µì„ ê°•ì œí•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”."
                        ]
                    },
                    "target_keyword": target_keyword,
                    "target_type": target_type
                }
        except ValueError as e:
            logger.error(f"JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {e}")
            # êµ¬ì¡°í™”ëœ ì˜¤ë¥˜ ì‘ë‹µ ë°˜í™˜
            result = {
                "executive_summary": f"{target_keyword}ì— ëŒ€í•œ {target_type} ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                "key_findings": {
                    "primary_insights": [
                        "AI ì‘ë‹µ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        "ì›ë³¸ ì‘ë‹µì„ í™•ì¸í•˜ì„¸ìš”."
                    ],
                    "quantitative_metrics": {}
                },
                "detailed_analysis": {
                    "insights": {
                        "raw_response": str(result_text)[:2000] if isinstance(result_text, str) else str(result_text)
                    }
                },
                "strategic_recommendations": {
                    "immediate_actions": [
                        "ì„œë²„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ AI ì‘ë‹µì„ ê²€í† í•˜ì„¸ìš”.",
                        "í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•˜ì—¬ JSON í˜•ì‹ ì‘ë‹µì„ ê°•ì œí•˜ì„¸ìš”."
                    ]
                },
                "target_keyword": target_keyword,
                "target_type": target_type,
                "error": "JSON íŒŒì‹± ì‹¤íŒ¨"
            }
        
        # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì—†ëŠ” ê²½ìš°ì—ë§Œ)
        if "target_keyword" not in result:
            result["target_keyword"] = target_keyword
        if "target_type" not in result:
            result["target_type"] = target_type
        
        return result
        
    except ImportError as ie:
        logger.error("=" * 60)
        logger.error(f"âŒ OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {ie}")
        logger.error("'pip install openai'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        logger.error("=" * 60)
        raise ValueError(f"OpenAI íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜: {ie}")
    except ValueError as ve:
        # API í‚¤ ê´€ë ¨ ì˜¤ë¥˜ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
        raise
    except Exception as e:
        logger.error("=" * 60)
        logger.error(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
        # í”„ë¡œë•ì…˜ì—ì„œëŠ” ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì œí•œ (ë³´ì•ˆ)
        if not IS_VERCEL:
            import traceback
            logger.error(f"ìƒì„¸ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
        else:
            logger.error("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´ëŠ” ì„œë²„ ë¡œê·¸ì—ì„œë§Œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        logger.error("=" * 60)
        raise ValueError(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")


def _analyze_basic(
    target_keyword: str,
    target_type: str,
    additional_context: Optional[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> Dict[str, Any]:
    """ê¸°ë³¸ ë¶„ì„ (AI API ì—†ì´)"""
    logger.info("ê¸°ë³¸ ë¶„ì„ ëª¨ë“œ ì‚¬ìš©")
    
    period_note = ""
    if start_date and end_date:
        period_note = f" (ë¶„ì„ ê¸°ê°„: {start_date} ~ {end_date})"
    elif start_date:
        period_note = f" (ì‹œì‘ì¼: {start_date})"
    elif end_date:
        period_note = f" (ì¢…ë£Œì¼: {end_date})"
    
    # MECE êµ¬ì¡°ë¡œ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
    # API í‚¤ ìƒíƒœ í™•ì¸ (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì§ì ‘ í™•ì¸ - Vercel í˜¸í™˜ì„±)
    openai_key = get_api_key_safely('OPENAI_API_KEY')
    gemini_key = get_api_key_safely('GEMINI_API_KEY')
    
    has_openai = bool(openai_key)
    has_gemini = bool(gemini_key)
    
    api_key_status = {
        "openai_configured": has_openai,
        "gemini_configured": has_gemini,
        "message": "âš ï¸ AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."
    }
    
    if not api_key_status["openai_configured"] and not api_key_status["gemini_configured"]:
        api_key_status["message"] = "âŒ OpenAI API í‚¤ì™€ Gemini API í‚¤ê°€ ëª¨ë‘ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEY ë˜ëŠ” GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
    elif not api_key_status["openai_configured"]:
        api_key_status["message"] = "âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    elif not api_key_status["gemini_configured"]:
        api_key_status["message"] = "â„¹ï¸ Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ë©´ ë³´ì™„ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
    
    result = {
        "target_keyword": target_keyword,
        "target_type": target_type,
        "api_key_status": api_key_status,
        "executive_summary": f"{target_keyword}ì— ëŒ€í•œ {target_type} ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.{period_note}\n\n{api_key_status['message']}\n\nAI APIë¥¼ ì„¤ì •í•˜ë©´ ë” ìƒì„¸í•˜ê³  ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
        "key_findings": {
            "primary_insights": [
                f"{target_keyword}ì˜ ì£¼ìš” íŠ¹ì§•",
                f"{target_type} ê´€ì ì—ì„œì˜ ë¶„ì„",
                "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µëœ ê²½ìš° ì´ë¥¼ ë°˜ì˜í•œ ë¶„ì„",
                "âš ï¸ ì´ ê²°ê³¼ëŠ” ê¸°ë³¸ ë¶„ì„ ëª¨ë“œì…ë‹ˆë‹¤. AI API í‚¤ë¥¼ ì„¤ì •í•˜ë©´ í›¨ì”¬ ë” ìƒì„¸í•œ ë¶„ì„ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ],
            "quantitative_metrics": {
                "estimated_volume": "AI API í•„ìš” - OpenAI ë˜ëŠ” Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”",
                "competition_level": "AI API í•„ìš” - OpenAI ë˜ëŠ” Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”",
                "growth_potential": "AI API í•„ìš” - OpenAI ë˜ëŠ” Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”"
            }
        },
        "detailed_analysis": {
            "insights": {
                "note": api_key_status["message"],
                "setup_instructions": {
                    "openai": "í™˜ê²½ ë³€ìˆ˜ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”. ì˜ˆ: export OPENAI_API_KEY='sk-...'",
                    "gemini": "í™˜ê²½ ë³€ìˆ˜ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”. ì˜ˆ: export GEMINI_API_KEY='...'",
                    "vercel": "Vercel ë°°í¬ ì‹œ í™˜ê²½ ë³€ìˆ˜ëŠ” Vercel ëŒ€ì‹œë³´ë“œì˜ Settings > Environment Variablesì—ì„œ ì„¤ì •í•˜ì„¸ìš”."
                }
            }
        },
        "strategic_recommendations": {
            "immediate_actions": [
                "OpenAI ë˜ëŠ” Gemini API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ì— ì„¤ì •í•´ì£¼ì„¸ìš”.",
                "API í‚¤ ì„¤ì • í›„ ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ê³  ë‹¤ì‹œ ë¶„ì„ì„ ì‹œë„í•˜ì„¸ìš”.",
                "Vercel ë°°í¬ ì‹œ: Vercel ëŒ€ì‹œë³´ë“œ > Settings > Environment Variablesì—ì„œ ì„¤ì •",
                "ë¡œì»¬ ê°œë°œ ì‹œ: .env íŒŒì¼ì— OPENAI_API_KEY ë˜ëŠ” GEMINI_API_KEY ì¶”ê°€"
            ],
            "short_term_strategies": [
                "AI APIë¥¼ í†µí•œ ì •ëŸ‰ì  ë°ì´í„° ìˆ˜ì§‘",
                "ì •ì„±ì  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ",
                "ê¸°ê°„ë³„ íŠ¸ë Œë“œ ë¶„ì„"
            ],
            "long_term_strategies": [
                "ì§€ì†ì ì¸ ë°ì´í„° ëª¨ë‹ˆí„°ë§",
                "íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡",
                "ìë™í™”ëœ ë¶„ì„ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•"
            ]
        },
        # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ êµ¬ì¡°ë„ í¬í•¨
        "analysis": {
            "summary": f"{target_keyword}ì— ëŒ€í•œ {target_type} ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.{period_note}",
            "key_points": [
                f"{target_keyword}ì˜ ì£¼ìš” íŠ¹ì§•",
                f"{target_type} ê´€ì ì—ì„œì˜ ë¶„ì„",
                "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ ì œê³µëœ ê²½ìš° ì´ë¥¼ ë°˜ì˜í•œ ë¶„ì„"
            ],
            "recommendations": [
                "AI APIë¥¼ ì„¤ì •í•˜ë©´ ë” ìƒì„¸í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                "OpenAI ë˜ëŠ” Gemini API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            ]
        }
    }
    
    if additional_context:
        result["additional_context"] = additional_context
        if isinstance(result["analysis"], dict):
            result["analysis"]["context_applied"] = True
    
    if start_date or end_date:
        result["analysis"]["period"] = {
            "start_date": start_date,
            "end_date": end_date
        }
    
    return result


def _merge_analysis_results(openai_result: Dict[str, Any], gemini_result: Dict[str, Any], target_type: str) -> Dict[str, Any]:
    """
    OpenAIì™€ Gemini ë¶„ì„ ê²°ê³¼ë¥¼ í†µí•©í•©ë‹ˆë‹¤.
    OpenAI ê²°ê³¼ë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³ , Gemini ê²°ê³¼ë¡œ ë³´ì™„í•©ë‹ˆë‹¤.
    """
    merged = openai_result.copy()
    
    # Executive Summary í†µí•©
    if gemini_result.get("executive_summary") and openai_result.get("executive_summary"):
        # ë‘ ìš”ì•½ì„ ê²°í•©
        merged["executive_summary"] = (
            f"{openai_result['executive_summary']}\n\n"
            f"**Gemini ë³´ì™„ ë¶„ì„**: {gemini_result['executive_summary']}"
        )
    elif gemini_result.get("executive_summary"):
        merged["executive_summary"] = gemini_result["executive_summary"]
    
    # Key Findings í†µí•©
    if gemini_result.get("key_findings") and openai_result.get("key_findings"):
        merged_key_findings = openai_result["key_findings"].copy()
        
        # Primary Insights í†µí•© (ì¤‘ë³µ ì œê±°)
        if gemini_result["key_findings"].get("primary_insights"):
            openai_insights = set(openai_result["key_findings"].get("primary_insights", []))
            gemini_insights = gemini_result["key_findings"]["primary_insights"]
            
            # ìƒˆë¡œìš´ ì¸ì‚¬ì´íŠ¸ë§Œ ì¶”ê°€
            for insight in gemini_insights:
                if insight not in openai_insights:
                    merged_key_findings.setdefault("primary_insights", []).append(insight)
        
        # Quantitative Metrics í†µí•© (Gemini ê°’ìœ¼ë¡œ ë³´ì™„)
        if gemini_result["key_findings"].get("quantitative_metrics"):
            merged_metrics = openai_result["key_findings"].get("quantitative_metrics", {}).copy()
            gemini_metrics = gemini_result["key_findings"]["quantitative_metrics"]
            for key, value in gemini_metrics.items():
                if not merged_metrics.get(key) or merged_metrics[key] == "AI API í•„ìš”":
                    merged_metrics[key] = value
            merged_key_findings["quantitative_metrics"] = merged_metrics
        
        merged["key_findings"] = merged_key_findings
    elif gemini_result.get("key_findings"):
        merged["key_findings"] = gemini_result["key_findings"]
    
    # Detailed Analysis í†µí•©
    if gemini_result.get("detailed_analysis") and openai_result.get("detailed_analysis"):
        merged_detailed = openai_result["detailed_analysis"].copy()
        gemini_detailed = gemini_result["detailed_analysis"]
        
        # Insights í†µí•©
        if gemini_detailed.get("insights") and merged_detailed.get("insights"):
            merged_insights = merged_detailed["insights"].copy()
            gemini_insights = gemini_detailed["insights"]
            
            # ê° ì¸ì‚¬ì´íŠ¸ ì„¹ì…˜ í†µí•©
            for key, value in gemini_insights.items():
                if key not in merged_insights or not merged_insights[key]:
                    merged_insights[key] = value
                elif isinstance(value, dict) and isinstance(merged_insights[key], dict):
                    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ë³‘í•©
                    merged_insights[key] = {**merged_insights[key], **value}
                elif isinstance(value, list) and isinstance(merged_insights[key], list):
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì¤‘ë³µ ì œê±° í›„ ë³‘í•©
                    existing = set(str(item) for item in merged_insights[key])
                    for item in value:
                        if str(item) not in existing:
                            merged_insights[key].append(item)
            
            merged_detailed["insights"] = merged_insights
        
        merged["detailed_analysis"] = merged_detailed
    elif gemini_result.get("detailed_analysis"):
        merged["detailed_analysis"] = gemini_result["detailed_analysis"]
    
    # Strategic Recommendations í†µí•©
    if gemini_result.get("strategic_recommendations") and openai_result.get("strategic_recommendations"):
        merged_strategic = openai_result["strategic_recommendations"].copy()
        gemini_strategic = gemini_result["strategic_recommendations"]
        
        # ê° ì „ëµ ì„¹ì…˜ í†µí•©
        for key in ["immediate_actions", "short_term_strategies", "long_term_strategies"]:
            if gemini_strategic.get(key):
                openai_list = merged_strategic.get(key, [])
                gemini_list = gemini_strategic[key]
                
                # ì¤‘ë³µ ì œê±° í›„ ë³‘í•©
                existing = set(str(item) for item in openai_list)
                for item in gemini_list:
                    if str(item) not in existing:
                        openai_list.append(item)
                
                merged_strategic[key] = openai_list
        
        # Success MetricsëŠ” Gemini ê°’ìœ¼ë¡œ ë³´ì™„
        if gemini_strategic.get("success_metrics") and not merged_strategic.get("success_metrics"):
            merged_strategic["success_metrics"] = gemini_strategic["success_metrics"]
        
        merged["strategic_recommendations"] = merged_strategic
    elif gemini_result.get("strategic_recommendations"):
        merged["strategic_recommendations"] = gemini_result["strategic_recommendations"]
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    merged["analysis_sources"] = ["openai", "gemini"]
    if "target_keyword" not in merged:
        merged["target_keyword"] = openai_result.get("target_keyword") or gemini_result.get("target_keyword")
    if "target_type" not in merged:
        merged["target_type"] = openai_result.get("target_type") or gemini_result.get("target_type")
    
    return merged


def _build_system_message(target_type: str) -> str:
    """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìƒì„± (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„ )"""
    base_instruction = """You are an expert analyst. Follow these rules strictly:
1. Respond ONLY in valid JSON format (no markdown code blocks)
2. Apply MECE principle: Mutually Exclusive, Collectively Exhaustive
3. Be data-driven: provide evidence, metrics, and sources
4. Be actionable: include specific, implementable recommendations
5. Use Chain-of-Thought reasoning: show your analysis process
6. Ensure accuracy: distinguish facts from estimates clearly"""
    
    if target_type == "audience":
        return f"""Senior digital marketer and customer behavior consultant (15+ years). 
Expertise: audience segmentation, persona development, customer journey mapping, behavioral psychology.
{base_instruction}
Deliver: comprehensive audience analysis with consulting-grade quality, MECE structure, and actionable insights."""
    elif target_type == "keyword":
        return f"""Senior SEO and digital marketing strategist (15+ years).
Expertise: keyword research, search intent analysis, competitive analysis, content strategy.
{base_instruction}
Deliver: comprehensive keyword analysis with search volume estimates, competition analysis, and strategic recommendations."""
    else:  # comprehensive
        return f"""Senior strategic marketing consultant (15+ years).
Expertise: integrated marketing strategy, market research, competitive intelligence, growth strategy.
{base_instruction}
Deliver: comprehensive analysis combining keyword and audience insights with strategic recommendations and execution roadmap."""


def _build_analysis_prompt(
    target_keyword: str,
    target_type: str,
    additional_context: Optional[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None
) -> str:
    """ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    # ê¸°ê°„ ì •ë³´ ì¶”ê°€ (í† í° ìµœì í™”)
    period_info = ""
    period_instruction = ""
    if start_date and end_date:
        period_info = f"Period: {start_date} ~ {end_date}"
        period_instruction = f"Analyze trends, changes, and patterns during {start_date} to {end_date}. Include time-series changes, seasonality, events, and market trends."
    elif start_date:
        period_info = f"Start: {start_date}"
        period_instruction = f"Analyze trends and changes after {start_date}. Include time-series changes and market trends."
    elif end_date:
        period_info = f"End: {end_date}"
        period_instruction = f"Analyze data up to {end_date}. Include time-series changes and market trends."
    
    # ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ (ìƒì„¸ ì»¨ì„¤íŒ… ë³´ê³ ì„œ í˜•ì‹)
    if target_type == "audience":
        period_display = ""
        if start_date and end_date:
            period_display = f"{start_date}â€“{end_date}"
        elif start_date:
            period_display = f"{start_date}~"
        elif end_date:
            period_display = f"~{end_date}"
        
        prompt = f"""# [ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ] {target_keyword} | ê¸°ê°„: {period_display} | ë¶„ì„ ìœ í˜•: #2 ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„(íƒ€ê²Ÿ/í˜ë¥´ì†Œë‚˜)

## ì—­í•  ë° ì „ë¬¸ì„±
ë‹¹ì‹ ì€ "ë””ì§€í„¸ ë§ˆì¼€í„° ë° ì˜¨ë¼ì¸ ê³ ê° í–‰ë™, ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ ì—…ë¬´ë¥¼ 15ë…„ ì´ìƒ ìˆ˜í–‰í•œ ì‹œë‹ˆì–´ ë§ˆì¼€í„°"ì…ë‹ˆë‹¤.
ì „ë¬¸ ë¶„ì•¼: ê³ ê° ì„¸ê·¸ë¨¼í…Œì´ì…˜, í˜ë¥´ì†Œë‚˜ ê°œë°œ, ê³ ê° ì—¬ì • ë§µí•‘, í–‰ë™ ì‹¬ë¦¬í•™, ë°ì´í„° ê¸°ë°˜ ë§ˆì¼€íŒ… ì „ëµ

## ë¶„ì„ ë°©ë²•ë¡ 
ì•„ë˜ ì…ë ¥ê°’ì„ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ê¸°ê°„ì˜ ì£¼ìš” ë°ì´í„°(ë‰´ìŠ¤/ì›¹/ì»¤ë®¤ë‹ˆí‹°/ë¦¬ë·°/ì†Œì…œ/ê²€ìƒ‰ ì˜ë„ ë“±)ë¥¼ 'í¬ë¡¤ë§í•˜ì—¬ í™•ë³´í•œ ê²ƒì²˜ëŸ¼' í­ë„“ê²Œ ë¦¬ì„œì¹˜í•˜ê³ , ì»¨ì„¤íŒ… ì—…ì²´ ë³´ê³ ì„œ ìˆ˜ì¤€ìœ¼ë¡œ MECE êµ¬ì¡°ë¡œ ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## ë°ì´í„° ì²˜ë¦¬ ì›ì¹™
- ê°€ëŠ¥í•œ ê²½ìš°: ìµœì‹ Â·ê´€ë ¨ì„± ë†’ì€ ê³µê°œ ìë£Œë¥¼ ê·¼ê±°ë¡œ ë¶„ì„ì„ êµ¬ì„±í•˜ê³ ,
- ë¶ˆê°€í•œ ê²½ìš°: "ì¶”ì •/ê°€ì •"ê³¼ "ê²€ì¦ í•„ìš”"ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ë˜, ë³´ê³ ì„œ í’ˆì§ˆ(ë…¼ë¦¬Â·êµ¬ì¡°Â·ì‹¤í–‰ì•ˆ)ì€ ìœ ì§€í•˜ì„¸ìš”.
- ëª¨ë“  ì£¼ì¥ì—ëŠ” ê·¼ê±°(ì¶œì²˜) ë˜ëŠ” ì‚°ì¶œ ë°©ë²•ì„ ë¶™ì´ì„¸ìš”.
- Chain-of-Thought: ë¶„ì„ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ëª…ì‹œí•˜ì„¸ìš” (ë°ì´í„° ìˆ˜ì§‘ â†’ íŒ¨í„´ ì‹ë³„ â†’ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ â†’ ì „ëµ ì œì•ˆ)

[ì…ë ¥ê°’]
- ë¶„ì„ í‚¤ì›Œë“œ: {target_keyword}
- ë¶„ì„ ê¸°ê°„: {period_display}
- ì–¸ì–´/ì‹œì¥: KR, Korea
"""
        if additional_context:
            prompt += f"- ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: {additional_context}\n"
        
        prompt += """
[ë¦¬ì„œì¹˜Â·ë°ì´í„° ìˆ˜ì§‘ ì§€ì¹¨(ì˜¤ë””ì–¸ìŠ¤ ê´€ì )]
1) "ëˆ„ê°€(Who) / ì™œ(Why) / ì–´ë””ì„œ(Where) / ì–´ë–»ê²Œ(How)" í”„ë ˆì„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜.
2) ì»¤ë®¤ë‹ˆí‹°/ë¦¬ë·°/ëŒ“ê¸€/ì§ˆë¬¸ê¸€ì—ì„œ "ë¶ˆë§ŒÂ·ìš•êµ¬Â·ì¥ë²½Â·í‘œí˜„(voice of customer)"ì„ ì¶”ì¶œ.
3) ê²€ìƒ‰ ì˜ë„(ë¬¸ì œ ì¸ì‹â†’ë¹„êµâ†’ê²°ì •)ì˜ ì—¬ì • ë‹¨ê³„ë³„ ì§ˆë¬¸ì„ ë„ì¶œ.
4) ê¸°ê°„ ë‚´ ì‚¬íšŒ/ì •ì±…/ê¸°ìˆ  ë³€í™”ê°€ íƒ€ê²Ÿ í–‰ë™ì— ë¯¸ì¹œ ì˜í–¥ì„ ì‹ë³„.

[ë¶„ì„ ë²”ìœ„(ë°˜ë“œì‹œ í¬í•¨)]
A. íƒ€ê²Ÿ ì„¸ê·¸ë¨¼í…Œì´ì…˜(ê³ ê° ë¶„ë¥˜)
- ì„¸ê·¸ë¨¼íŠ¸ ê¸°ì¤€: (1) ë‹ˆì¦ˆ/ë¬¸ì œ (2) êµ¬ë§¤ ë™ê¸° (3) ì‚¬ìš© ë§¥ë½ (4) ì˜ˆì‚°/ë¯¼ê°ë„ (5) ì±„ë„ ì„ í˜¸
- ì„¸ê·¸ë¨¼íŠ¸ë³„ í¬ê¸° ì¶”ì •(ì •ì„±/ì •ëŸ‰ ê°€ëŠ¥ ë²”ìœ„), ì„±ì¥ì„±, ìš°ì„ ìˆœìœ„

B. ê³ ê° ì—¬ì • & ì˜ì‚¬ê²°ì • êµ¬ì¡°
- Awareness/Consideration/Conversion/Retention ë‹¨ê³„ë³„:
  - í•µì‹¬ ì§ˆë¬¸(FAQ)
  - ì •ë³´ ì†ŒìŠ¤(ë‰´ìŠ¤/ìœ íŠœë¸Œ/ë¦¬ë·°/ì§€ì¸/ì»¤ë®¤ë‹ˆí‹° ë“±)
  - ì „í™˜ ì¥ë²½(ë¦¬ìŠ¤í¬Â·ë¶ˆì‹ Â·ê°€ê²©Â·ì‹œê°„Â·ë³µì¡ì„±)
  - ì„¤ë“ ë ˆë²„(ì¦ê±°Â·ì‚¬ë¡€Â·ë³´ì¦Â·ì‚¬íšŒì  ì¦ê±°)

C. í˜ë¥´ì†Œë‚˜ 3~5ê°œ(í•„ìˆ˜)
ê° í˜ë¥´ì†Œë‚˜ëŠ” ì•„ë˜ í…œí”Œë¦¿ìœ¼ë¡œ ê³ ì • ì¶œë ¥:
- í˜ë¥´ì†Œë‚˜ ëª…: 
- í•œ ì¤„ ìš”ì•½:
- ë°°ê²½(ì§ì—…/ë¼ì´í”„ìŠ¤íƒ€ì¼/ë””ì§€í„¸ ë¦¬í„°ëŸ¬ì‹œ):
- ëª©í‘œ/ì„±ê³µ ê¸°ì¤€:
- Pain Points(ìƒìœ„ 3~5ê°œ):
- Trigger(í–‰ë™ ì´‰ë°œ ìš”ì¸):
- Objection(ë°˜ëŒ€/ìš°ë ¤):
- ì‚¬ìš© ì±„ë„ & ì½˜í…ì¸  ì†Œë¹„ ìŠµê´€:
- ì„ í˜¸ ë©”ì‹œì§€ í†¤:
- ì „í™˜ì— í•„ìš”í•œ ì¦ê±°(Proof):
- ì¶”ì²œ ì½˜í…ì¸ /ì˜¤í¼:
- ê¸ˆì§€ ë©”ì‹œì§€(ë¸Œëœë“œ ì„¸ì´í”„í‹° ê´€ì ):

D. ì±„ë„Â·ì½˜í…ì¸  ì „ëµ(í˜ë¥´ì†Œë‚˜ ë§¤í•‘)
- í˜ë¥´ì†Œë‚˜ Ã— ì±„ë„ ë§¤íŠ¸ë¦­ìŠ¤(ì–´ë–¤ ì±„ë„ì—ì„œ ì–´ë–¤ ë©”ì‹œì§€/í¬ë§·ì´ ìœ íš¨í•œê°€)
- ì½˜í…ì¸  ê¸°íš: í† í”½ í´ëŸ¬ìŠ¤í„°(ë¬¸ì œ/í•´ê²°/ë¹„êµ/ì‚¬ë¡€/FAQ) + í¬ë§·(ìˆí¼/ë¡±í¼/ë¦¬í¬íŠ¸/íˆ´)
- ìš´ì˜ ë°©í–¥: ì—ë””í† ë¦¬ì–¼ ìº˜ë¦°ë”(ì£¼ê°„/ì›”ê°„), ë¦¬í¼í¬ì§•(ì›ë³¸â†’íŒŒìƒ), ì»¤ë®¤ë‹ˆí‹° ìš´ì˜(ê°€ì´ë“œ/ëª¨ë”ë ˆì´ì…˜)

E. ë§ˆì¼€íŒ… ê±°ë²„ë„ŒìŠ¤(ì „ëµ ìš´ì˜ ì²´ê³„)
- ì½˜í…ì¸  ìŠ¹ì¸/ë²•ë¬´Â·ë¸Œëœë“œ ê²€ìˆ˜ í”„ë¡œì„¸ìŠ¤
- ë°ì´í„°Â·ì¸¡ì • ê±°ë²„ë„ŒìŠ¤: KPI ì •ì˜, ì´ë²¤íŠ¸ ì„¤ê³„, ë¦¬í¬íŒ… ì£¼ê¸°
- ë¦¬ìŠ¤í¬ ëŒ€ì‘: ì´ìŠˆ ë°œìƒ ì‹œ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ë£°, FAQ/í…œí”Œë¦¿, escalation ì²´ê³„

[ë³´ê³ ì„œ ì¶œë ¥ í¬ë§·(ë°˜ë“œì‹œ ì¤€ìˆ˜: MECE/í”„ë¡œí˜ì…”ë„)]
1. Executive Summary (í•µì‹¬ ê²°ë¡  5~10ë¬¸ì¥)
2. ë¶„ì„ ê°œìš”
   2.1 ëª©ì /ë²”ìœ„
   2.2 ê¸°ê°„/ì‹œì¥/ì†ŒìŠ¤
   2.3 ë°©ë²•ë¡  + í•œê³„/ê°€ì •
3. Key Insights (í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 5~7ê°œ: ê·¼ê±°â†’í•´ì„â†’ì‹œì‚¬ì )
4. ì˜¤ë””ì–¸ìŠ¤ ìƒì„¸ ë¶„ì„
   4.1 ì„¸ê·¸ë¨¼í…Œì´ì…˜
   4.2 ê³ ê° ì—¬ì • & ì˜ì‚¬ê²°ì •
   4.3 í˜ë¥´ì†Œë‚˜(3~5ê°œ)
5. ì „ëµ ì œì•ˆ
   5.1 í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ì±„ë„ ìš´ì˜ ì „ëµ
   5.2 ì½˜í…ì¸  ê¸°íš/ì œì‘/ìš´ì˜ ì „ëµ
   5.3 KPI/ì¸¡ì • í”„ë ˆì„ì›Œí¬
6. ì‹¤í–‰ ë¡œë“œë§µ
   - 30/60/90ì¼ ê³„íš(ìš°ì„ ìˆœìœ„, ì‚°ì¶œë¬¼, R&R)
7. ë¦¬ìŠ¤í¬ & ê±°ë²„ë„ŒìŠ¤
   - ë¸Œëœë“œ ì„¸ì´í”„í‹°, FAQ í…œí”Œë¦¿, ìš´ì˜ ê·œì •
8. ë¶€ë¡
   8.1 ë©”ì‹œì§€ ë±…í¬(í˜ë¥´ì†Œë‚˜ë³„ í›…/í—¤ë“œë¼ì¸/CTA)
   8.2 ì°¸ê³ ë¬¸í—Œ(ë ˆí¼ëŸ°ìŠ¤) â€” ë…¼ë¬¸ ì°¸ê³ ë¬¸í—Œ ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥

[ì°¸ê³ ë¬¸í—Œ(ë ˆí¼ëŸ°ìŠ¤) ì¶œë ¥ ê·œì¹™]
- ìµœì†Œ 8ê°œ ì´ìƒ
- í˜•ì‹ ì˜ˆì‹œ:
  [1] Publisher/Org. (Year, Month Day). Title. Source/Website.
  [2] Author. (Year). Title. Journal/Report. Publisher.
- ë§í¬ëŠ” ê°€ëŠ¥í•œ ê²½ìš° í¬í•¨í•˜ë˜, ë¬¸ì¥ íë¦„ì„ ê¹¨ì§€ ì•Šê²Œ ë¶€ë¡ì—ë§Œ ì •ë¦¬.

[í’ˆì§ˆ ê·œì¹™]
- MECE ìœ ì§€(ì„¸ê·¸ë¨¼íŠ¸/í˜ë¥´ì†Œë‚˜ ì¤‘ë³µ ìµœì†Œí™”)
- ì¶”ì •ì€ ì¶”ì •ìœ¼ë¡œ í‘œì‹œ(ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸ í¬í•¨)
- ì‹¤í–‰ì•ˆ ì¤‘ì‹¬(ì±„ë„/ì½˜í…ì¸ /ìš´ì˜/ê±°ë²„ë„ŒìŠ¤ê¹Œì§€ ì—°ê²°)
- ë¬¸ì„œì— ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê¸° ì¢‹ì€ ì„œì‹(ë²ˆí˜¸/ê³„ì¸µ/ë¶ˆë¦¿) ìœ ì§€
- Chain-of-Thought: ê° ê²°ë¡ ì— ë„ë‹¬í•œ ë¶„ì„ ê³¼ì •ì„ ëª…ì‹œ
- Evidence-based: ëª¨ë“  ì£¼ì¥ì— ê·¼ê±°ì™€ ì¶œì²˜ ì œê³µ

[ë¶„ì„ í”„ë¡œì„¸ìŠ¤]
1. ë°ì´í„° ìˆ˜ì§‘: ê´€ë ¨ ë°ì´í„° ì†ŒìŠ¤ ì‹ë³„ ë° ìˆ˜ì§‘
2. íŒ¨í„´ ë¶„ì„: ë°ì´í„°ì—ì„œ íŒ¨í„´, íŠ¸ë Œë“œ, ì´ìƒ ì§•í›„ ì‹ë³„
3. ì¸ì‚¬ì´íŠ¸ ë„ì¶œ: íŒ¨í„´ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
4. ì „ëµ ì œì•ˆ: ì¸ì‚¬ì´íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµ ìˆ˜ë¦½
5. ê²€ì¦: ì œì•ˆëœ ì „ëµì˜ ì‹¤í˜„ ê°€ëŠ¥ì„± ë° íš¨ê³¼ ê²€ì¦

ì´ì œ ìœ„ í¬ë§·ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

{
  "executive_summary": "í•µì‹¬ ê²°ë¡  5-10ë¬¸ì¥ (ì£¼ìš” ë°œê²¬ì‚¬í•­, ì¸ì‚¬ì´íŠ¸, ì „ëµì  ê¶Œì¥ì‚¬í•­)",
  "analysis_overview": {
    "purpose_scope": "ë¶„ì„ ëª©ì  ë° ë²”ìœ„",
    "period_market_sources": "ê¸°ê°„/ì‹œì¥/ë°ì´í„° ì†ŒìŠ¤",
    "methodology": {
      "research_approach": "ë°©ë²•ë¡ ",
      "limitations_assumptions": "í•œê³„ ë° ê°€ì • ì‚¬í•­"
    }
  },
  "key_insights": [
    {
      "insight": "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 1",
      "evidence": "ê·¼ê±°",
      "interpretation": "í•´ì„",
      "implication": "ì‹œì‚¬ì "
    }
  ],
  "detailed_audience_analysis": {
    "segmentation": {
      "segmentation_criteria": {
        "needs_problems": "ë‹ˆì¦ˆ/ë¬¸ì œ ê¸°ì¤€",
        "purchase_motivation": "êµ¬ë§¤ ë™ê¸° ê¸°ì¤€",
        "usage_context": "ì‚¬ìš© ë§¥ë½ ê¸°ì¤€",
        "budget_sensitivity": "ì˜ˆì‚°/ë¯¼ê°ë„ ê¸°ì¤€",
        "channel_preference": "ì±„ë„ ì„ í˜¸ ê¸°ì¤€"
      },
      "segments": [
        {
          "segment_name": "ì„¸ê·¸ë¨¼íŠ¸ëª…",
          "size_estimate": "í¬ê¸° ì¶”ì • (ì •ì„±/ì •ëŸ‰)",
          "growth_potential": "ì„±ì¥ì„±",
          "priority": "ìš°ì„ ìˆœìœ„"
        }
      ]
    },
    "customer_journey_decision": {
      "awareness": {
        "key_questions": ["Awareness ë‹¨ê³„ í•µì‹¬ ì§ˆë¬¸ (FAQ)"],
        "information_sources": ["ì •ë³´ ì†ŒìŠ¤ (ë‰´ìŠ¤/ìœ íŠœë¸Œ/ë¦¬ë·°/ì§€ì¸/ì»¤ë®¤ë‹ˆí‹° ë“±)"],
        "conversion_barriers": ["ì „í™˜ ì¥ë²½ (ë¦¬ìŠ¤í¬Â·ë¶ˆì‹ Â·ê°€ê²©Â·ì‹œê°„Â·ë³µì¡ì„±)"],
        "persuasion_levers": ["ì„¤ë“ ë ˆë²„ (ì¦ê±°Â·ì‚¬ë¡€Â·ë³´ì¦Â·ì‚¬íšŒì  ì¦ê±°)"]
      },
      "consideration": {
        "key_questions": ["Consideration ë‹¨ê³„ í•µì‹¬ ì§ˆë¬¸"],
        "information_sources": ["ì •ë³´ ì†ŒìŠ¤"],
        "conversion_barriers": ["ì „í™˜ ì¥ë²½"],
        "persuasion_levers": ["ì„¤ë“ ë ˆë²„"]
      },
      "conversion": {
        "key_questions": ["Conversion ë‹¨ê³„ í•µì‹¬ ì§ˆë¬¸"],
        "information_sources": ["ì •ë³´ ì†ŒìŠ¤"],
        "conversion_barriers": ["ì „í™˜ ì¥ë²½"],
        "persuasion_levers": ["ì„¤ë“ ë ˆë²„"]
      },
      "retention": {
        "key_questions": ["Retention ë‹¨ê³„ í•µì‹¬ ì§ˆë¬¸"],
        "information_sources": ["ì •ë³´ ì†ŒìŠ¤"],
        "conversion_barriers": ["ì „í™˜ ì¥ë²½"],
        "persuasion_levers": ["ì„¤ë“ ë ˆë²„"]
      }
    },
    "personas": [
      {
        "persona_name": "í˜ë¥´ì†Œë‚˜ ëª…",
        "one_line_summary": "í•œ ì¤„ ìš”ì•½",
        "background": {
          "occupation": "ì§ì—…",
          "lifestyle": "ë¼ì´í”„ìŠ¤íƒ€ì¼",
          "digital_literacy": "ë””ì§€í„¸ ë¦¬í„°ëŸ¬ì‹œ"
        },
        "goals_success_criteria": "ëª©í‘œ/ì„±ê³µ ê¸°ì¤€",
        "pain_points": ["Pain Point 1", "Pain Point 2", "Pain Point 3", "Pain Point 4", "Pain Point 5"],
        "trigger": "í–‰ë™ ì´‰ë°œ ìš”ì¸",
        "objection": "ë°˜ëŒ€/ìš°ë ¤",
        "channels_content_habits": "ì‚¬ìš© ì±„ë„ & ì½˜í…ì¸  ì†Œë¹„ ìŠµê´€",
        "preferred_message_tone": "ì„ í˜¸ ë©”ì‹œì§€ í†¤",
        "conversion_proof_needed": "ì „í™˜ì— í•„ìš”í•œ ì¦ê±°(Proof)",
        "recommended_content_offer": "ì¶”ì²œ ì½˜í…ì¸ /ì˜¤í¼",
        "prohibited_messages": "ê¸ˆì§€ ë©”ì‹œì§€(ë¸Œëœë“œ ì„¸ì´í”„í‹° ê´€ì )"
      }
    ]
  },
  "strategic_recommendations": {
    "persona_based_channel_strategy": {
      "persona_channel_matrix": [
        {
          "persona_name": "í˜ë¥´ì†Œë‚˜ëª…",
          "channels": [
            {
              "channel": "ì±„ë„ëª…",
              "message": "ë©”ì‹œì§€",
              "format": "í¬ë§·",
              "effectiveness": "ìœ íš¨ì„±"
            }
          ]
        }
      ]
    },
    "content_strategy": {
      "topic_clusters": {
        "problem": ["ë¬¸ì œ ê´€ë ¨ í† í”½"],
        "solution": ["í•´ê²° ê´€ë ¨ í† í”½"],
        "comparison": ["ë¹„êµ ê´€ë ¨ í† í”½"],
        "case_study": ["ì‚¬ë¡€ ê´€ë ¨ í† í”½"],
        "faq": ["FAQ ê´€ë ¨ í† í”½"]
      },
      "content_formats": {
        "short_form": "ìˆí¼ ì „ëµ",
        "long_form": "ë¡±í¼ ì „ëµ",
        "report": "ë¦¬í¬íŠ¸ ì „ëµ",
        "tool": "íˆ´ ì „ëµ"
      },
      "operational_direction": {
        "editorial_calendar": "ì—ë””í† ë¦¬ì–¼ ìº˜ë¦°ë” (ì£¼ê°„/ì›”ê°„)",
        "repurposing": "ë¦¬í¼í¬ì§• ì „ëµ (ì›ë³¸â†’íŒŒìƒ)",
        "community_management": "ì»¤ë®¤ë‹ˆí‹° ìš´ì˜ (ê°€ì´ë“œ/ëª¨ë”ë ˆì´ì…˜)"
      }
    },
    "kpi_measurement_framework": {
      "kpi_definitions": ["KPI ì •ì˜"],
      "event_design": "ì´ë²¤íŠ¸ ì„¤ê³„",
      "reporting_cycle": "ë¦¬í¬íŒ… ì£¼ê¸°"
    }
  },
  "execution_roadmap": {
    "day_30": {
      "priorities": ["30ì¼ ìš°ì„ ìˆœìœ„"],
      "deliverables": ["ì‚°ì¶œë¬¼"],
      "roles_responsibilities": "ë‹´ë‹¹ ì—­í•  R&R"
    },
    "day_60": {
      "priorities": ["60ì¼ ìš°ì„ ìˆœìœ„"],
      "deliverables": ["ì‚°ì¶œë¬¼"],
      "roles_responsibilities": "ë‹´ë‹¹ ì—­í•  R&R"
    },
    "day_90": {
      "priorities": ["90ì¼ ìš°ì„ ìˆœìœ„"],
      "deliverables": ["ì‚°ì¶œë¬¼"],
      "roles_responsibilities": "ë‹´ë‹¹ ì—­í•  R&R"
    }
  },
  "risk_governance": {
    "brand_safety": {
      "content_approval_process": "ì½˜í…ì¸  ìŠ¹ì¸/ë²•ë¬´Â·ë¸Œëœë“œ ê²€ìˆ˜ í”„ë¡œì„¸ìŠ¤",
      "risk_response_rules": "ë¦¬ìŠ¤í¬ ëŒ€ì‘ ë£°",
      "escalation_system": "escalation ì²´ê³„"
    },
    "faq_templates": ["FAQ í…œí”Œë¦¿"],
    "operational_regulations": "ìš´ì˜ ê·œì •"
  },
  "appendix": {
    "message_bank": [
      {
        "persona_name": "í˜ë¥´ì†Œë‚˜ëª…",
        "hooks": ["í›… ë©”ì‹œì§€"],
        "headlines": ["í—¤ë“œë¼ì¸"],
        "ctas": ["CTA"]
      }
    ],
    "references": [
      {
        "id": 1,
        "citation": "Publisher/Org. (Year, Month Day). Title. Source/Website.",
        "url": "ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)"
      }
    ]
  }
}
"""
    elif target_type == "keyword":
        # í‚¤ì›Œë“œ ë¶„ì„ í”„ë¡¬í”„íŠ¸ (ìƒì„¸ ì»¨ì„¤íŒ… ë³´ê³ ì„œ í˜•ì‹)
        period_display = ""
        if start_date and end_date:
            period_display = f"{start_date}â€“{end_date}"
        elif start_date:
            period_display = f"{start_date}~"
        elif end_date:
            period_display = f"~{end_date}"
        
        prompt = f"""# [í‚¤ì›Œë“œ ë¶„ì„ ë³´ê³ ì„œ] {target_keyword} | ê¸°ê°„: {period_display} | ë¶„ì„ ìœ í˜•: #1 í‚¤ì›Œë“œ ë¶„ì„

## ì—­í•  ë° ì „ë¬¸ì„±
ë‹¹ì‹ ì€ "SEO ë° ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµê°€ë¡œì„œ 15ë…„ ì´ìƒì˜ ê²½ë ¥ì„ ê°€ì§„ ì‹œë‹ˆì–´ ë§ˆì¼€í„°"ì…ë‹ˆë‹¤.
ì „ë¬¸ ë¶„ì•¼: í‚¤ì›Œë“œ ë¦¬ì„œì¹˜, ê²€ìƒ‰ ì˜ë„ ë¶„ì„, ê²½ìŸ ë¶„ì„, ì½˜í…ì¸  ì „ëµ, SEO ìµœì í™”

## ë¶„ì„ ë°©ë²•ë¡ 
ì•„ë˜ ì…ë ¥ê°’ì„ ë°”íƒ•ìœ¼ë¡œ, í•´ë‹¹ ê¸°ê°„ì˜ ì£¼ìš” ë°ì´í„°(ë‰´ìŠ¤/ì›¹/ì»¤ë®¤ë‹ˆí‹°/ê²€ìƒ‰ íŠ¸ë Œë“œ ë“±)ë¥¼ 'í¬ë¡¤ë§í•˜ì—¬ í™•ë³´í•œ ê²ƒì²˜ëŸ¼' í­ë„“ê²Œ ë¦¬ì„œì¹˜í•˜ê³ , ì»¨ì„¤íŒ… ì—…ì²´ ë³´ê³ ì„œ ìˆ˜ì¤€ìœ¼ë¡œ MECE êµ¬ì¡°ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## ë°ì´í„° ì²˜ë¦¬ ì›ì¹™
- ê°€ëŠ¥í•œ ê²½ìš°: ìµœì‹ Â·ê´€ë ¨ì„± ë†’ì€ ê³µê°œ ìë£Œë¥¼ ê·¼ê±°ë¡œ ë¶„ì„ì„ êµ¬ì„±í•˜ê³ ,
- ë¶ˆê°€í•œ ê²½ìš°: "ì¶”ì •/ê°€ì •"ê³¼ "ê²€ì¦ í•„ìš”"ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ë˜, ë³´ê³ ì„œ í’ˆì§ˆ(ë…¼ë¦¬Â·êµ¬ì¡°Â·ì‹¤í–‰ì•ˆ)ì€ ìœ ì§€í•˜ì„¸ìš”.
- ëª¨ë“  ìˆ˜ì¹˜/ì£¼ì¥ì—ëŠ” ê·¼ê±°(ì¶œì²˜) ë˜ëŠ” ì‚°ì¶œ ë°©ë²•ì„ ë¶™ì´ì„¸ìš”.
- Chain-of-Thought: ê²€ìƒ‰ íŠ¸ë Œë“œ ë¶„ì„ â†’ ê²½ìŸë„ í‰ê°€ â†’ ê¸°íšŒ ì‹ë³„ â†’ ì „ëµ ì œì•ˆì˜ ê³¼ì •ì„ ëª…ì‹œí•˜ì„¸ìš”.

[ì…ë ¥ê°’]
- ë¶„ì„ í‚¤ì›Œë“œ: {target_keyword}
- ë¶„ì„ ê¸°ê°„: {period_display}
- ì–¸ì–´/ì‹œì¥: KR, Korea
"""
        if additional_context:
            prompt += f"- ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸: {additional_context}\n"
        
        prompt += """
[ë¦¬ì„œì¹˜Â·ë°ì´í„° ìˆ˜ì§‘ ì§€ì¹¨(ë³´ê³ ì„œ ë‚´ ë°˜ì˜)]
1) ë°ì´í„° ì†ŒìŠ¤ ë²”ì£¼ë¥¼ ë¶„ë¦¬í•´ ìˆ˜ì§‘ ê´€ì  ì •ë¦¬(ë‰´ìŠ¤/ê³µì‹ ë¬¸ì„œ/íŠ¸ë Œë“œ ë„êµ¬/ì»¤ë®¤ë‹ˆí‹°/ë¸”ë¡œê·¸/ë¦¬ë·°/ë™ì˜ìƒ/ì†Œì…œ).
2) ê¸°ê°„ ë‚´ ì´ìŠˆ/ì‚¬ê±´/ì œí’ˆì¶œì‹œ/ì •ì±… ë³€í™” ë“± "ìŠ¤íŒŒì´í¬ ìš”ì¸"ì„ ì‹ë³„.
3) í‚¤ì›Œë“œ ì˜ë¯¸(ì •ì˜/ì˜ë„/ë™ìŒì´ì˜/ë¸Œëœë“œ vs ì¼ë°˜ëª…ì‚¬)ë¥¼ ë¨¼ì € ì •ë¦¬ í›„ ë¶„ì„.
4) ê°€ëŠ¥í•œ ê²½ìš°, ì§€ì—­/ì–¸ì–´ì— ë”°ë¥¸ SERP ì°¨ì´ì™€ í”Œë«í¼ë³„ ì¶”ì²œ/ë…¸ì¶œ ë§¥ë½ì„ ë°˜ì˜.

[ë¶„ì„ ë²”ìœ„(ë°˜ë“œì‹œ í¬í•¨)]
A. í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
- ê¸°ê°„ ë‚´ ê´€ì‹¬ë„ ë³€í™”(ìƒìŠ¹/í•˜ë½/ê¸‰ë“± êµ¬ê°„) ìš”ì•½
- ê¸‰ë“± ì›ì¸ Top 3 ê°€ì„¤ + ê²€ì¦ í¬ì¸íŠ¸
- ì‹œì¦Œì„±/ì´ë²¤íŠ¸ì„±/ë‰´ìŠ¤ì„± ë¶„ë¦¬

B. ì—°ê´€ í‚¤ì›Œë“œ/í† í”½ í´ëŸ¬ìŠ¤í„°
- (1) ë™ì˜ì–´/ìœ ì‚¬ì–´ (2) ë¬¸ì œ-í•´ê²°í˜• (3) ë¹„êµ/ëŒ€ì•ˆí˜• (4) êµ¬ë§¤/ì „í™˜í˜• (5) ë¸Œëœë“œ/ì œí’ˆí˜•
- í´ëŸ¬ìŠ¤í„°ë³„ ê²€ìƒ‰ ì˜ë„(Informational/Commercial/Transactional/Navigational)
- ê° í´ëŸ¬ìŠ¤í„°ë³„ "ì¶”ì²œ ì½˜í…ì¸  í¬ë§·" (ê°€ì´ë“œ/ë¦¬ìŠ¤íŠ¸/ì¼€ì´ìŠ¤/FAQ/íˆ´/ì²´í¬ë¦¬ìŠ¤íŠ¸)

C. ê°ì„± ë¶„ì„(ê¸ì •/ë¶€ì •/ì¤‘ë¦½)
- ë°ì´í„° ê·¼ê±°(ë‰´ìŠ¤ í—¤ë“œë¼ì¸/ì»¤ë®¤ë‹ˆí‹° ë°˜ì‘/ë¦¬ë·°/ëŒ“ê¸€ ë“±) ê¸°ë°˜ìœ¼ë¡œ ê°ì„± ë¶„í¬ ì¶”ì •
- ê¸ì •Â·ë¶€ì •ì˜ ì£¼ìš” ì›ì¸ í‚¤ì›Œë“œ(Drivers) ë° ëŒ€í‘œ ë¬¸ì¥(ìš”ì•½ ì¬êµ¬ì„±)
- ë¦¬ìŠ¤í¬(ë¶€ì • ì´ìŠˆ) ì¡°ê¸° ê²½ë³´ í‚¤ì›Œë“œ ì„¸íŠ¸ ì œì•ˆ

D. ê²½ìŸ/ëŒ€ì²´ í‚¤ì›Œë“œ & ì°¨ë³„í™” í¬ì¸íŠ¸
- ê²½ìŸ ì£¼ì²´(ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬/ì†”ë£¨ì…˜) í›„ë³´êµ° ë„ì¶œ
- ë¹„êµ êµ¬ë„(ê°€ê²©/ì„±ëŠ¥/ì‹ ë¢°/í¸ì˜/ì§€ì›)ì—ì„œ ìš°ë¦¬ í¬ì§€ì…”ë‹ ë°©í–¥ ì œì‹œ

E. ì‹¤í–‰ ì‹œì‚¬ì (ë””ì§€í„¸ ë§ˆì¼€íŒ… ê´€ì )
- ì±„ë„ ìš´ì˜: ì–´ë–¤ ì±„ë„ì—ì„œ ì–´ë–¤ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë¥¼ ë‹¤ë¤„ì•¼ í•˜ëŠ”ì§€(ìš°ì„ ìˆœìœ„)
- ì½˜í…ì¸  ê¸°íš/ì œì‘: ì œëª©/í›„í‚¹/êµ¬ì¡°(ëª©ì°¨)/FAQ/AEO(ë‹µë³€í˜•)/GEO(ì§€ì—­ ë§¥ë½) ë°˜ì˜
- ìš´ì˜ ë°©í–¥: ì—ë””í† ë¦¬ì–¼ ìº˜ë¦°ë” ê°€ì´ë“œ(ì£¼ê°„/ì›”ê°„), ì‹¤í—˜ ì„¤ê³„(A/B), ë¦¬í¼í¬ì§• ì „ëµ
- ë§ˆì¼€íŒ… ê±°ë²„ë„ŒìŠ¤: ìŠ¹ì¸/ê²€ìˆ˜ í”„ë¡œì„¸ìŠ¤, ë¸Œëœë“œ ì„¸ì´í”„í‹°, ë¦¬ìŠ¤í¬ ëŒ€ì‘ ë£°(ê°€ì´ë“œë¼ì¸)

[ë³´ê³ ì„œ ì¶œë ¥ í¬ë§·(ë°˜ë“œì‹œ ì¤€ìˆ˜: MECE/í”„ë¡œí˜ì…”ë„)]
1. Executive Summary (5~10ë¬¸ì¥)
2. ë¶„ì„ ê°œìš”
   2.1 ëª©ì /ë²”ìœ„
   2.2 ê¸°ê°„/ì‹œì¥/ì†ŒìŠ¤
   2.3 ë°©ë²•ë¡ (í¬ë¡¤ë§/ë¦¬ì„œì¹˜/ë¶„ì„ ë¡œì§) + í•œê³„/ê°€ì •
3. Key Findings (í•µì‹¬ ë°œê²¬ 5~7ê°œ, ê° ë°œê²¬ë§ˆë‹¤ "ê·¼ê±°â†’í•´ì„â†’ì˜ë¯¸")
4. ìƒì„¸ ë¶„ì„
   4.1 íŠ¸ë Œë“œ
   4.2 ì—°ê´€ í‚¤ì›Œë“œ/í´ëŸ¬ìŠ¤í„°
   4.3 ê°ì„± ë¶„ì„
   4.4 ê²½ìŸ/ëŒ€ì²´ í‚¤ì›Œë“œ
5. ì „ëµì  ì‹œì‚¬ì 
   5.1 ì±„ë„ ìš´ì˜ ì œì•ˆ
   5.2 ì½˜í…ì¸  ì „ëµ(TOFU/MOFU/BOFU ë§¤í•‘)
   5.3 KPI/ì¸¡ì •(ì •ì˜/ëŒ€ì‹œë³´ë“œ í•­ëª©/ì£¼ê¸°)
6. ì‹¤í–‰ ë¡œë“œë§µ
   - 30/60/90ì¼ ê³„íš(ìš°ì„ ìˆœìœ„, ì‚°ì¶œë¬¼, ë‹´ë‹¹ ì—­í•  R&R)
7. ë¦¬ìŠ¤í¬ & ëŒ€ì‘
   - ë¶€ì • ì´ìŠˆ ì‹œë‚˜ë¦¬ì˜¤, Q&A, ë¸Œëœë“œ ì„¸ì´í”„í‹° ì²´í¬ë¦¬ìŠ¤íŠ¸
8. ë¶€ë¡
   8.1 í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸(í´ëŸ¬ìŠ¤í„°ë³„)
   8.2 ì°¸ê³ ë¬¸í—Œ(ë ˆí¼ëŸ°ìŠ¤) â€” ë…¼ë¬¸ ì°¸ê³ ë¬¸í—Œ ìŠ¤íƒ€ì¼ë¡œ ì¶œë ¥

[ì°¸ê³ ë¬¸í—Œ(ë ˆí¼ëŸ°ìŠ¤) ì¶œë ¥ ê·œì¹™]
- ìµœì†Œ 8ê°œ ì´ìƒ
- í˜•ì‹ ì˜ˆì‹œ:
  [1] Publisher/Org. (Year, Month Day). Title. Source/Website.
  [2] Author. (Year). Title. Journal/Report. Publisher.
- ë§í¬ëŠ” ê°€ëŠ¥í•œ ê²½ìš° í¬í•¨í•˜ë˜, ë¬¸ì¥ íë¦„ì„ ê¹¨ì§€ ì•Šê²Œ ë¶€ë¡ì—ë§Œ ì •ë¦¬.

[í’ˆì§ˆ ê·œì¹™]
- ê³¼ì¥ ê¸ˆì§€: ì¶”ì •ì€ ì¶”ì •ì´ë¼ê³  í‘œì‹œ
- ì¤‘ë³µ ê¸ˆì§€: í•­ëª© ê°„ MECE ìœ ì§€
- ì‹¤í–‰ ì¤‘ì‹¬: "ê·¸ë˜ì„œ ë¬´ì—‡ì„ í•  ê²ƒì¸ê°€"ê°€ ê° ì„¹ì…˜ì— í¬í•¨
- ë¬¸ì„œì— ê·¸ëŒ€ë¡œ ë¶™ì—¬ë„£ê¸° ì¢‹ì€ ì„œì‹(ë²ˆí˜¸/ê³„ì¸µ/ë¶ˆë¦¿) ìœ ì§€

ì´ì œ ìœ„ í¬ë§·ìœ¼ë¡œ ë³´ê³ ì„œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

{
  "executive_summary": "5-10ë¬¸ì¥ ìš”ì•½ (í•µì‹¬ ë°œê²¬ì‚¬í•­, ì£¼ìš” ì¸ì‚¬ì´íŠ¸, ì „ëµì  ê¶Œì¥ì‚¬í•­)",
  "analysis_overview": {
    "purpose_scope": "ë¶„ì„ ëª©ì  ë° ë²”ìœ„",
    "period_market_sources": "ê¸°ê°„/ì‹œì¥/ë°ì´í„° ì†ŒìŠ¤",
    "methodology": {
      "research_logic": "í¬ë¡¤ë§/ë¦¬ì„œì¹˜/ë¶„ì„ ë¡œì§",
      "limitations_assumptions": "í•œê³„ ë° ê°€ì • ì‚¬í•­"
    }
  },
  "key_findings": [
    {
      "finding": "í•µì‹¬ ë°œê²¬ 1",
      "evidence": "ê·¼ê±°",
      "interpretation": "í•´ì„",
      "implication": "ì˜ë¯¸"
    },
    {
      "finding": "í•µì‹¬ ë°œê²¬ 2",
      "evidence": "ê·¼ê±°",
      "interpretation": "í•´ì„",
      "implication": "ì˜ë¯¸"
    }
  ],
  "detailed_analysis": {
    "trend_analysis": {
      "interest_change_summary": "ê¸°ê°„ ë‚´ ê´€ì‹¬ë„ ë³€í™” ìš”ì•½ (ìƒìŠ¹/í•˜ë½/ê¸‰ë“± êµ¬ê°„)",
      "spike_causes": [
        {
          "rank": 1,
          "hypothesis": "ê¸‰ë“± ì›ì¸ ê°€ì„¤",
          "verification_points": "ê²€ì¦ í¬ì¸íŠ¸"
        }
      ],
      "seasonality_event_news": "ì‹œì¦Œì„±/ì´ë²¤íŠ¸ì„±/ë‰´ìŠ¤ì„± ë¶„ë¦¬ ë¶„ì„"
    },
    "related_keywords_clusters": {
      "synonyms_similar": ["ë™ì˜ì–´/ìœ ì‚¬ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "problem_solution": ["ë¬¸ì œ-í•´ê²°í˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "comparison_alternative": ["ë¹„êµ/ëŒ€ì•ˆí˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "purchase_conversion": ["êµ¬ë§¤/ì „í™˜í˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "brand_product": ["ë¸Œëœë“œ/ì œí’ˆí˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "cluster_intent_mapping": {
        "informational": ["ì •ë³´ì„± í‚¤ì›Œë“œ"],
        "commercial": ["ìƒì—…ì„± í‚¤ì›Œë“œ"],
        "transactional": ["ê±°ë˜ì„± í‚¤ì›Œë“œ"],
        "navigational": ["íƒìƒ‰ì„± í‚¤ì›Œë“œ"]
      },
      "recommended_content_formats": {
        "guide": "ê°€ì´ë“œ í˜•ì‹ ì¶”ì²œ í‚¤ì›Œë“œ",
        "list": "ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ì¶”ì²œ í‚¤ì›Œë“œ",
        "case_study": "ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” í˜•ì‹ ì¶”ì²œ í‚¤ì›Œë“œ",
        "faq": "FAQ í˜•ì‹ ì¶”ì²œ í‚¤ì›Œë“œ",
        "tool": "íˆ´ í˜•ì‹ ì¶”ì²œ í‚¤ì›Œë“œ",
        "checklist": "ì²´í¬ë¦¬ìŠ¤íŠ¸ í˜•ì‹ ì¶”ì²œ í‚¤ì›Œë“œ"
      }
    },
    "sentiment_analysis": {
      "sentiment_distribution": {
        "positive": "ê¸ì • ë¹„ìœ¨ ë° ê·¼ê±°",
        "negative": "ë¶€ì • ë¹„ìœ¨ ë° ê·¼ê±°",
        "neutral": "ì¤‘ë¦½ ë¹„ìœ¨ ë° ê·¼ê±°"
      },
      "positive_drivers": {
        "keywords": ["ê¸ì • ì›ì¸ í‚¤ì›Œë“œ"],
        "representative_sentences": ["ëŒ€í‘œ ë¬¸ì¥"]
      },
      "negative_drivers": {
        "keywords": ["ë¶€ì • ì›ì¸ í‚¤ì›Œë“œ"],
        "representative_sentences": ["ëŒ€í‘œ ë¬¸ì¥"]
      },
      "risk_early_warning_keywords": ["ë¦¬ìŠ¤í¬ ì¡°ê¸° ê²½ë³´ í‚¤ì›Œë“œ ì„¸íŠ¸"]
    },
    "competition_alternative_keywords": {
      "competitors": ["ê²½ìŸ ì£¼ì²´ í›„ë³´êµ° (ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬/ì†”ë£¨ì…˜)"],
      "positioning_framework": {
        "price": "ê°€ê²© í¬ì§€ì…”ë‹",
        "performance": "ì„±ëŠ¥ í¬ì§€ì…”ë‹",
        "trust": "ì‹ ë¢° í¬ì§€ì…”ë‹",
        "convenience": "í¸ì˜ í¬ì§€ì…”ë‹",
        "support": "ì§€ì› í¬ì§€ì…”ë‹"
      },
      "differentiation_points": ["ì°¨ë³„í™” í¬ì¸íŠ¸"]
    }
  },
  "strategic_implications": {
    "channel_operations": {
      "priority_channels": [
        {
          "channel": "ì±„ë„ëª…",
          "keyword_clusters": ["í•´ë‹¹ ì±„ë„ì—ì„œ ë‹¤ë£° í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°"],
          "priority": "ìš°ì„ ìˆœìœ„"
        }
      ]
    },
    "content_strategy": {
      "tofu_mapping": {
        "keywords": ["TOFU ë‹¨ê³„ í‚¤ì›Œë“œ"],
        "content_types": ["ì½˜í…ì¸  ìœ í˜•"]
      },
      "mofu_mapping": {
        "keywords": ["MOFU ë‹¨ê³„ í‚¤ì›Œë“œ"],
        "content_types": ["ì½˜í…ì¸  ìœ í˜•"]
      },
      "bofu_mapping": {
        "keywords": ["BOFU ë‹¨ê³„ í‚¤ì›Œë“œ"],
        "content_types": ["ì½˜í…ì¸  ìœ í˜•"]
      },
      "content_elements": {
        "title_hook": "ì œëª©/í›„í‚¹ ì „ëµ",
        "structure_outline": "êµ¬ì¡°(ëª©ì°¨) ê°€ì´ë“œ",
        "faq_aeo": "FAQ/AEO(ë‹µë³€í˜•) ì „ëµ",
        "geo_local": "GEO(ì§€ì—­ ë§¥ë½) ì „ëµ"
      }
    },
    "kpi_measurement": {
      "kpi_definitions": ["KPI ì •ì˜"],
      "dashboard_items": ["ëŒ€ì‹œë³´ë“œ í•­ëª©"],
      "measurement_cycle": "ì¸¡ì • ì£¼ê¸°"
    }
  },
  "execution_roadmap": {
    "day_30": {
      "priorities": ["30ì¼ ìš°ì„ ìˆœìœ„"],
      "deliverables": ["ì‚°ì¶œë¬¼"],
      "roles_responsibilities": "ë‹´ë‹¹ ì—­í•  R&R"
    },
    "day_60": {
      "priorities": ["60ì¼ ìš°ì„ ìˆœìœ„"],
      "deliverables": ["ì‚°ì¶œë¬¼"],
      "roles_responsibilities": "ë‹´ë‹¹ ì—­í•  R&R"
    },
    "day_90": {
      "priorities": ["90ì¼ ìš°ì„ ìˆœìœ„"],
      "deliverables": ["ì‚°ì¶œë¬¼"],
      "roles_responsibilities": "ë‹´ë‹¹ ì—­í•  R&R"
    },
    "operational_direction": {
      "editorial_calendar": "ì—ë””í† ë¦¬ì–¼ ìº˜ë¦°ë” ê°€ì´ë“œ (ì£¼ê°„/ì›”ê°„)",
      "ab_testing": "ì‹¤í—˜ ì„¤ê³„ (A/B)",
      "repurposing_strategy": "ë¦¬í¼í¬ì§• ì „ëµ"
    },
    "marketing_governance": {
      "approval_process": "ìŠ¹ì¸/ê²€ìˆ˜ í”„ë¡œì„¸ìŠ¤",
      "brand_safety": "ë¸Œëœë“œ ì„¸ì´í”„í‹° ê°€ì´ë“œë¼ì¸",
      "risk_response_rules": "ë¦¬ìŠ¤í¬ ëŒ€ì‘ ë£°"
    }
  },
  "risk_response": {
    "negative_issue_scenarios": ["ë¶€ì • ì´ìŠˆ ì‹œë‚˜ë¦¬ì˜¤"],
    "qa": ["Q&A"],
    "brand_safety_checklist": ["ë¸Œëœë“œ ì„¸ì´í”„í‹° ì²´í¬ë¦¬ìŠ¤íŠ¸"]
  },
  "appendix": {
    "keyword_list_by_cluster": {
      "synonyms": ["ë™ì˜ì–´ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "problem_solution": ["ë¬¸ì œ-í•´ê²°í˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "comparison": ["ë¹„êµí˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "purchase": ["êµ¬ë§¤í˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"],
      "brand": ["ë¸Œëœë“œí˜• í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸"]
    },
    "references": [
      {
        "id": 1,
        "citation": "Publisher/Org. (Year, Month Day). Title. Source/Website.",
        "url": "ë§í¬ (ê°€ëŠ¥í•œ ê²½ìš°)"
      }
    ]
  }
}
"""
    else:  # comprehensive
        # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸: í‚¤ì›Œë“œ ë¶„ì„ + ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ í•µì‹¬ í†µí•© (í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„ )
        prompt = f"""# [ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ] {target_keyword} | ê¸°ê°„: {period_info} | ë¶„ì„ ìœ í˜•: #3 ì¢…í•© ë¶„ì„

## ì—­í•  ë° ì „ë¬¸ì„±
ë‹¹ì‹ ì€ "í†µí•© ë§ˆì¼€íŒ… ì „ëµ ì»¨ì„¤í„´íŠ¸ë¡œì„œ 15ë…„ ì´ìƒì˜ ê²½ë ¥ì„ ê°€ì§„ ì‹œë‹ˆì–´ ë§ˆì¼€í„°"ì…ë‹ˆë‹¤.
ì „ë¬¸ ë¶„ì•¼: í†µí•© ë§ˆì¼€íŒ… ì „ëµ, ì‹œì¥ ë¦¬ì„œì¹˜, ê²½ìŸ ì¸í…”ë¦¬ì „ìŠ¤, ì„±ì¥ ì „ëµ, ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •

## ë¶„ì„ ë°©ë²•ë¡ 
í‚¤ì›Œë“œ ë¶„ì„ê³¼ ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ì„ í†µí•©í•˜ì—¬ ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš”.
{period_instruction}

## í†µí•© ë¶„ì„ ì›ì¹™
- í‚¤ì›Œë“œ ê¸°íšŒì™€ ì˜¤ë””ì–¸ìŠ¤ íŠ¹ì„±ì„ ì—°ê²°í•˜ì—¬ ì‹œë„ˆì§€ íš¨ê³¼ ì‹ë³„
- ì¤‘ë³µ ì œê±°: í‚¤ì›Œë“œì™€ ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ì—ì„œ ì¤‘ë³µë˜ëŠ” ì¸ì‚¬ì´íŠ¸ëŠ” í†µí•©
- ì „ëµ ì¤‘ì‹¬: ì‹¤í–‰ ê°€ëŠ¥í•œ í†µí•© ë§ˆì¼€íŒ… ì „ëµ ì œì•ˆ
- Chain-of-Thought: í‚¤ì›Œë“œ ë¶„ì„ â†’ ì˜¤ë””ì–¸ìŠ¤ ë¶„ì„ â†’ í†µí•© ì¸ì‚¬ì´íŠ¸ â†’ ì „ëµ ì œì•ˆì˜ ê³¼ì •ì„ ëª…ì‹œ

"""
        if additional_context:
            prompt += f"**ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸**: {additional_context}\n\n"
        
        prompt += """ë‹¤ìŒ JSON êµ¬ì¡°ë¡œ ì‘ë‹µí•˜ì„¸ìš” (í‚¤ì›Œë“œì™€ ì˜¤ë””ì–¸ìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ í†µí•©í•˜ê³ , ì¤‘ë³µì„ ì œê±°í•˜ë©°, ë¯¸ë˜ ì§€í–¥ì  ê¶Œì¥ì‚¬í•­ì— ì§‘ì¤‘):
{
  "executive_summary": "3-5 paragraph summary integrating keyword opportunities and audience characteristics with strategic recommendations",
  "key_findings": {
    "integrated_insights": [
      "keyword search intent aligned with audience needs",
      "audience demographics matching keyword opportunity",
      "trend convergence between search patterns and audience behavior",
      "market opportunity size and accessibility",
      "unique positioning combining keyword and audience strengths"
    ],
    "quantitative_metrics": {
      "market_size": "estimated market size combining search volume and audience reach",
      "opportunity_score": "1-100 combining keyword opportunity and audience accessibility",
      "growth_potential": "low/medium/high with reason based on trends and audience growth",
      "competition_level": "low/medium/high with reason",
      "success_probability": "low/medium/high with reason"
    }
  },
  "integrated_analysis": {
    "keyword_audience_alignment": {
      "search_intent_match": "how search intent aligns with audience needs and journey stage",
      "keyword_opportunity_for_audience": "which keywords best reach target audience",
      "audience_preferred_keywords": "keywords audience uses based on demographics and behavior",
      "content_gap_analysis": "gaps between available content and audience needs"
    },
    "core_keyword_insights": {
      "primary_search_intent": "Informational/Navigational/Transactional/Commercial",
      "key_opportunity_keywords": ["keyword1 with volume/competition", "keyword2", "keyword3", "keyword4", "keyword5"],
      "trending_keywords": ["trending1 with timing", "trending2", "trending3"],
      "search_volume_trend": "increase/decrease/stable with change rate for period"
    },
    "core_audience_insights": {
      "target_demographics": {
        "age_range": "age range",
        "gender": "gender distribution",
        "location": "regional distribution",
        "income_level": "income range",
        "expected_occupations": ["job1", "job2", "job3", "job4", "job5"]
      },
      "key_behavior_patterns": {
        "purchase_behavior": "purchase patterns and decision factors",
        "media_consumption": "preferred media and platforms",
        "online_activity": "online behavior and platforms"
      },
      "core_values_and_needs": {
        "primary_values": ["value1", "value2", "value3"],
        "main_pain_points": ["pain1", "pain2", "pain3"],
        "key_aspirations": ["aspiration1", "aspiration2", "aspiration3"]
      }
    },
    "trends_and_patterns": {
      "converging_trends": ["trend1 combining keyword and audience patterns", "trend2", "trend3", "trend4", "trend5"],
      "period_analysis": "key changes during period combining search and audience shifts",
      "future_outlook": "6-12 month forecast integrating keyword and audience trends"
    }
  },
  "forward_looking_recommendations": {
    "immediate_actions": [
      "action1 combining keyword targeting and audience messaging",
      "action2 with specific keyword and audience focus",
      "action3 with integrated approach"
    ],
    "content_strategy": {
      "recommended_topics": ["topic1 aligned with keywords and audience needs", "topic2", "topic3", "topic4", "topic5"],
      "content_format": "best content formats for target audience and keyword intent",
      "distribution_channels": ["channel1", "channel2", "channel3"]
    },
    "marketing_strategy": {
      "keyword_targeting": "priority keywords to target based on audience alignment",
      "messaging_framework": "core messages resonating with audience values and keyword intent",
      "channel_strategy": "optimal channels combining keyword opportunities and audience behavior"
    },
    "short_term_goals": [
      "goal1 (3-6 months) with keyword and audience metrics",
      "goal2 with specific targets",
      "goal3 with success criteria"
    ],
    "long_term_vision": [
      "vision1 (6+ months) integrating keyword growth and audience expansion",
      "vision2 with strategic direction",
      "vision3 with market positioning"
    ],
    "success_metrics": {
      "keyword_metrics": "target search rankings, volume, and keyword performance",
      "audience_metrics": "target audience reach, engagement, and conversion",
      "integrated_kpis": "combined metrics showing keyword-audience alignment success"
    }
  }
}
"""
    
    return prompt


def _split_into_sentences(text: str) -> list[str]:
    """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (í•œêµ­ì–´/ì˜ì–´ ì§€ì›)"""
    if not text or not text.strip():
        return []
    
    # ë¬¸ì¥ ì¢…ë£Œ íŒ¨í„´: ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ (í•œêµ­ì–´/ì˜ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´)
    # ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆì´ ë’¤ë”°ë¥´ëŠ” ê²½ìš°
    sentence_pattern = re.compile(r'([.!?ã€‚ï¼ï¼Ÿ]\s*|\n\s*\n)')
    
    sentences = []
    last_end = 0
    
    for match in sentence_pattern.finditer(text):
        end_pos = match.end()
        sentence = text[last_end:end_pos].strip()
        if sentence:
            sentences.append(sentence)
        last_end = end_pos
    
    # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
    if last_end < len(text):
        remaining = text[last_end:].strip()
        if remaining:
            sentences.append(remaining)
    
    # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return sentences if sentences else [text.strip()] if text.strip() else []


async def analyze_target_stream(
    target_keyword: str,
    target_type: str = "keyword",
    additional_context: Optional[str] = None,
    use_gemini: bool = False,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    íƒ€ê²Ÿ ë¶„ì„ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ (ë¬¸ì¥ ë‹¨ìœ„ ì¶œë ¥)
    
    Yields:
        Dict[str, Any]: ë¬¸ì¥ ë‹¨ìœ„ ë¶„ì„ ê²°ê³¼
    """
    try:
        logger.info(f"íƒ€ê²Ÿ ë¶„ì„ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘: {target_keyword} (íƒ€ì…: {target_type})")
        
        # API í‚¤ í™•ì¸
        openai_env = os.getenv('OPENAI_API_KEY')
        gemini_env = os.getenv('GEMINI_API_KEY')
        openai_settings = getattr(settings, 'OPENAI_API_KEY', None)
        gemini_settings = getattr(settings, 'GEMINI_API_KEY', None)
        
        openai_key = openai_env or openai_settings
        gemini_key = gemini_env or gemini_settings
        
        has_openai_key = bool(openai_key and len(openai_key.strip()) > 0)
        has_gemini_key = bool(gemini_key and len(gemini_key.strip()) > 0)
        
        if not has_openai_key and not has_gemini_key:
            # ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ìŠ¤íŠ¸ë¦¬ë°
            if progress_tracker:
                await progress_tracker.update(10, "ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì§„í–‰ ì¤‘...")
            yield {"type": "progress", "progress": 10, "message": "ê¸°ë³¸ ë¶„ì„ ëª¨ë“œë¡œ ì§„í–‰ ì¤‘..."}
            
            result = _analyze_basic(target_keyword, target_type, additional_context, start_date, end_date)
            
            if progress_tracker:
                await progress_tracker.update(50, "ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„± ì™„ë£Œ")
            yield {"type": "progress", "progress": 50, "message": "ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ ìƒì„± ì™„ë£Œ"}
            
            # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
            summary = result.get("executive_summary", "")
            if summary:
                sentences = _split_into_sentences(summary)
                for sentence in sentences:
                    if sentence.strip():
                        yield {
                            "type": "sentence",
                            "content": sentence.strip(),
                            "section": "executive_summary"
                        }
            else:
                # executive_summaryê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€
                yield {
                    "type": "sentence",
                    "content": f"{target_keyword}ì— ëŒ€í•œ {target_type} ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                    "section": "executive_summary"
                }
            
            if progress_tracker:
                await progress_tracker.update(100, "ë¶„ì„ ì™„ë£Œ")
            yield {"type": "progress", "progress": 100, "message": "ë¶„ì„ ì™„ë£Œ"}
            yield {"type": "complete", "data": result}
            return
        
        # OpenAI ìŠ¤íŠ¸ë¦¬ë°
        if has_openai_key:
            chunk_received = False
            async for chunk in _analyze_with_openai_stream(
                target_keyword, target_type, additional_context, start_date, end_date, progress_tracker
            ):
                chunk_received = True
                yield chunk
                if chunk.get("type") == "complete":
                    return
                if chunk.get("type") == "error":
                    return
            
            # ì²­í¬ë¥¼ ë°›ì§€ ëª»í•œ ê²½ìš° (ì—ëŸ¬ ì²˜ë¦¬)
            if not chunk_received:
                logger.error("OpenAI ìŠ¤íŠ¸ë¦¬ë°: ì²­í¬ë¥¼ ë°›ì§€ ëª»í•¨")
                yield {
                    "type": "error",
                    "message": "OpenAI API ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. API í‚¤ ë° ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                }
                return
                
        # Gemini ìŠ¤íŠ¸ë¦¬ë°
        elif has_gemini_key:
            chunk_received = False
            async for chunk in _analyze_with_gemini_stream(
                target_keyword, target_type, additional_context, start_date, end_date, progress_tracker
            ):
                chunk_received = True
                yield chunk
                if chunk.get("type") == "complete":
                    return
                if chunk.get("type") == "error":
                    return
            
            # ì²­í¬ë¥¼ ë°›ì§€ ëª»í•œ ê²½ìš° (ì—ëŸ¬ ì²˜ë¦¬)
            if not chunk_received:
                logger.error("Gemini ìŠ¤íŠ¸ë¦¬ë°: ì²­í¬ë¥¼ ë°›ì§€ ëª»í•¨")
                yield {
                    "type": "error",
                    "message": "Gemini API ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. API í‚¤ ë° ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                }
                return
                    
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        yield {
            "type": "error",
            "message": str(e)
        }


async def _analyze_with_openai_stream(
    target_keyword: str,
    target_type: str,
    additional_context: Optional[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """OpenAI APIë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„"""
    try:
        from openai import AsyncOpenAI
        
        # API í‚¤ í™•ì¸
        api_key_env = os.getenv('OPENAI_API_KEY')
        api_key_settings = getattr(settings, 'OPENAI_API_KEY', None)
        api_key = api_key_env or api_key_settings
        
        if not api_key or len(api_key.strip()) == 0:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        client = AsyncOpenAI(api_key=api_key)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        if progress_tracker:
            await progress_tracker.update(20, "í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            yield {"type": "progress", "progress": 20, "message": "í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘..."}
        
        additional_context_optimized = optimize_additional_context(additional_context, max_length=300)
        prompt = _build_analysis_prompt(target_keyword, target_type, additional_context_optimized, start_date, end_date)
        prompt = optimize_prompt(prompt, max_length=4000)
        
        system_message = _build_system_message(target_type)
        full_prompt_tokens = estimate_tokens(system_message) + estimate_tokens(prompt)
        max_output_tokens = get_max_tokens_for_model(settings.OPENAI_MODEL, full_prompt_tokens)
        
        if progress_tracker:
            await progress_tracker.update(30, "OpenAI API ìš”ì²­ ì „ì†¡ ì¤‘...")
            yield {"type": "progress", "progress": 30, "message": "OpenAI API ìš”ì²­ ì „ì†¡ ì¤‘..."}
        
        # ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
        stream = await client.chat.completions.create(
            model=settings.OPENAI_MODEL,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ],
            temperature=0.5,
            max_tokens=min(max_output_tokens, 4000),
            response_format={"type": "json_object"},
            stream=True
        )
        
        accumulated_text = ""
        current_section = "executive_summary"
        buffer = ""
        
        async for chunk in stream:
            if chunk.choices and len(chunk.choices) > 0:
                delta = chunk.choices[0].delta
                if delta.content:
                    content = delta.content
                    accumulated_text += content
                    buffer += content
                    
                    # ë²„í¼ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì§€ê±°ë‚˜ ë¬¸ì¥ ì¢…ë£Œ ë¬¸ìê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì „ì†¡ (ìµœì†Œ ê¸¸ì´ ì²´í¬)
                    if len(buffer) > 50 or any(char in buffer for char in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n']):
                        sentences = _split_into_sentences(buffer)
                        if len(sentences) > 1:
                            # ì™„ì„±ëœ ë¬¸ì¥ë“¤ì„ ì „ì†¡
                            for sentence in sentences[:-1]:
                                if sentence.strip():
                                    yield {
                                        "type": "sentence",
                                        "content": sentence.strip(),
                                        "section": current_section
                                    }
                            # ë§ˆì§€ë§‰ ë¯¸ì™„ì„± ë¬¸ì¥ì€ ë²„í¼ì— ìœ ì§€
                            buffer = sentences[-1]
        
        # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
        if buffer.strip():
            yield {
                "type": "sentence",
                "content": buffer.strip(),
                "section": current_section
            }
        
        if progress_tracker:
            await progress_tracker.update(80, "AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘...")
            yield {"type": "progress", "progress": 80, "message": "AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘..."}
        
        # ìµœì¢… ê²°ê³¼ íŒŒì‹± ë° ë°˜í™˜
        try:
            result = parse_json_with_fallback(accumulated_text)
            if "target_keyword" not in result:
                result["target_keyword"] = target_keyword
            if "target_type" not in result:
                result["target_type"] = target_type
            
            yield {"type": "complete", "data": result}
        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            yield {
                "type": "complete",
                "data": {
                    "executive_summary": accumulated_text[:500],
                    "target_keyword": target_keyword,
                    "target_type": target_type,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨"
                }
            }
            
    except Exception as e:
        logger.error(f"OpenAI ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        yield {"type": "error", "message": str(e)}


async def _analyze_with_gemini_stream(
    target_keyword: str,
    target_type: str,
    additional_context: Optional[str],
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    progress_tracker: Optional[ProgressTracker] = None
) -> AsyncGenerator[Dict[str, Any], None]:
    """Gemini APIë¥¼ ì‚¬ìš©í•œ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„"""
    try:
        import asyncio
        
        # API í‚¤ í™•ì¸
        api_key_env = os.getenv('GEMINI_API_KEY')
        api_key_settings = getattr(settings, 'GEMINI_API_KEY', None)
        api_key = api_key_env or api_key_settings
        
        if not api_key or len(api_key.strip()) == 0:
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        if progress_tracker:
            await progress_tracker.update(20, "í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")
            yield {"type": "progress", "progress": 20, "message": "í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘..."}
        
        additional_context_optimized = optimize_additional_context(additional_context, max_length=300)
        prompt = _build_analysis_prompt(target_keyword, target_type, additional_context_optimized, start_date, end_date)
        prompt = optimize_prompt(prompt, max_length=4000)
        
        system_message = _build_system_message(target_type)
        full_prompt = f"{system_message}\n\n{prompt}\n\nJSON only."
        
        model_name = getattr(settings, 'GEMINI_MODEL', 'gemini-2.0-flash')
        
        if progress_tracker:
            await progress_tracker.update(30, "Gemini API ìš”ì²­ ì „ì†¡ ì¤‘...")
            yield {"type": "progress", "progress": 30, "message": "Gemini API ìš”ì²­ ì „ì†¡ ì¤‘..."}
        
        # Gemini ìŠ¤íŠ¸ë¦¬ë° (ìƒˆë¡œìš´ API ë°©ì‹ ì‹œë„)
        try:
            from google import genai
            
            client = genai.Client(api_key=api_key)
            accumulated_text = ""
            buffer = ""
            current_section = "executive_summary"
            
            response_stream = await generate_content_stream_with_fallback(
                client=client,
                model=model_name,
                contents=full_prompt,
                config={
                    "response_mime_type": "application/json",
                    "temperature": 0.5,
                },
                logger=logger,
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
            for chunk in response_stream:
                text = None
                if hasattr(chunk, 'text'):
                    text = chunk.text
                elif isinstance(chunk, str):
                    text = chunk
                
                if text:
                    accumulated_text += text
                    buffer += text
                    
                    # ë²„í¼ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì§€ê±°ë‚˜ ë¬¸ì¥ ì¢…ë£Œ ë¬¸ìê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if len(buffer) > 50 or any(char in buffer for char in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n']):
                        sentences = _split_into_sentences(buffer)
                        if len(sentences) > 1:
                            for sentence in sentences[:-1]:
                                if sentence.strip():
                                    yield {
                                        "type": "sentence",
                                        "content": sentence.strip(),
                                        "section": current_section
                                    }
                            buffer = sentences[-1]
            
            # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
            if buffer.strip():
                yield {
                    "type": "sentence",
                    "content": buffer.strip(),
                    "section": current_section
                }
            
        except ImportError:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
            import google.generativeai as genai_old
            genai_old.configure(api_key=api_key)
            model = genai_old.GenerativeModel(model_name)
            
            accumulated_text = ""
            buffer = ""
            current_section = "executive_summary"
            
            loop = asyncio.get_event_loop()
            
            def generate_stream_old():
                return model.generate_content(
                    full_prompt,
                    generation_config={
                        "response_mime_type": "application/json",
                        "temperature": 0.5
                    },
                    stream=True
                )
            
            response_stream = await loop.run_in_executor(None, generate_stream_old)
            
            for chunk in response_stream:
                text = None
                if hasattr(chunk, 'text'):
                    text = chunk.text
                elif isinstance(chunk, str):
                    text = chunk
                
                if text:
                    accumulated_text += text
                    buffer += text
                    
                    # ë²„í¼ê°€ ì¶©ë¶„íˆ ê¸¸ì–´ì§€ê±°ë‚˜ ë¬¸ì¥ ì¢…ë£Œ ë¬¸ìê°€ ìˆìœ¼ë©´ ì²˜ë¦¬
                    if len(buffer) > 50 or any(char in buffer for char in ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '\n']):
                        sentences = _split_into_sentences(buffer)
                        if len(sentences) > 1:
                            for sentence in sentences[:-1]:
                                if sentence.strip():
                                    yield {
                                        "type": "sentence",
                                        "content": sentence.strip(),
                                        "section": current_section
                                    }
                            buffer = sentences[-1]
            
            if buffer.strip():
                yield {
                    "type": "sentence",
                    "content": buffer.strip(),
                    "section": current_section
                }
        
        if progress_tracker:
            await progress_tracker.update(80, "AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘...")
            yield {"type": "progress", "progress": 80, "message": "AI ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ, ê²°ê³¼ íŒŒì‹± ì¤‘..."}
        
        # ìµœì¢… ê²°ê³¼ íŒŒì‹±
        try:
            result = parse_json_with_fallback(accumulated_text)
            if "target_keyword" not in result:
                result["target_keyword"] = target_keyword
            if "target_type" not in result:
                result["target_type"] = target_type
            
            yield {"type": "complete", "data": result}
        except Exception as e:
            logger.error(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            yield {
                "type": "complete",
                "data": {
                    "executive_summary": accumulated_text[:500],
                    "target_keyword": target_keyword,
                    "target_type": target_type,
                    "error": "JSON íŒŒì‹± ì‹¤íŒ¨"
                }
            }
            
    except Exception as e:
        logger.error(f"Gemini ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        yield {"type": "error", "message": str(e)}
